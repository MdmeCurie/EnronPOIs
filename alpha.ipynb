{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHALEY DAVID A 18\n",
      "WROBEL BRUCE 18\n",
      "LOCKHART EUGENE E 20\n",
      "THE TRAVEL AGENCY IN THE PARK 18\n",
      "GRAMM WENDY L 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wendy/.local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# %load poi_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\". # You will need to use more features\n",
    "##### 'email_address' fails featureFormat() as it is string not float, all other features imported for \n",
    "##### initial testing and investigations\n",
    "features_list = ['poi', 'salary', 'bonus','deferral_payments','total_payments',  \n",
    "                 'exercised_stock_options','restricted_stock','total_stock_value',\n",
    "                 'expenses','deferred_income',\n",
    "                 'long_term_incentive', 'other',\n",
    "                 'restricted_stock_deferred', 'loan_advances', 'director_fees', \n",
    "                 'to_messages', 'shared_receipt_with_poi','from_messages',      \n",
    "                 'from_this_person_to_poi', 'from_poi_to_this_person'             \n",
    "                ] \n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "    \n",
    "for namen in data_dict:\n",
    "    count = 0\n",
    "    for things, values in data_dict[namen].items():\n",
    "        if values == 'NaN':\n",
    "            count +=1\n",
    "    if count >=18:\n",
    "        print namen, count\n",
    "        \n",
    "### Task 2: Remove outliers - remove TOTAL, Lockhardt Eugene E (all NaNs), and The Agency in the Park    \n",
    "outlier = data_dict.pop('TOTAL')\n",
    "outlier = data_dict.pop('LOCKHART EUGENE E')\n",
    "outlier = data_dict.pop('THE TRAVEL AGENCY IN THE PARK')\n",
    "\n",
    "### Negative outliers found in deferred_income and restricted stock deferred\n",
    "### Entries for 'BELFER ROBERT' & 'BHATANGAR SANJAY' corrected as confirmed by enron61702insiderpay.pdf\n",
    "\n",
    "data_dict['BELFER ROBERT']['deferred_income']  = -102500\n",
    "data_dict['BELFER ROBERT']['deferral_payments']= 'NaN'\n",
    "data_dict['BELFER ROBERT']['expenses'] = 3285\n",
    "data_dict['BELFER ROBERT']['director_fees'] = 102500 \n",
    "data_dict['BELFER ROBERT']['total_payments'] = 3285\n",
    "data_dict['BELFER ROBERT']['exercised_stock_options']= 'NaN'\n",
    "data_dict['BELFER ROBERT']['restricted_stock']= 44093\n",
    "data_dict['BELFER ROBERT']['restricted_stock_deferred']= -44093\n",
    "data_dict['BELFER ROBERT']['total_stock_value'] = 'NaN'\n",
    "\n",
    "data_dict['BHATNAGAR SANJAY']['other']= 'NaN' \n",
    "data_dict['BHATNAGAR SANJAY']['expenses']= 137864\n",
    "data_dict['BHATNAGAR SANJAY']['director_fees']= 'NaN'\n",
    "data_dict['BHATNAGAR SANJAY']['total_payments']= 137864\n",
    "data_dict['BHATNAGAR SANJAY']['exercised_stock_options']= 15456290 \n",
    "data_dict['BHATNAGAR SANJAY']['restricted_stock']= 2604490\n",
    "data_dict['BHATNAGAR SANJAY']['restricted_stock_deferred']= -2604490\n",
    "data_dict['BHATNAGAR SANJAY']['total_stock_value']= 15456290\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "\n",
    "## create dataframe of dict with features as cols and names as index\n",
    "#https://stackoverflow.com/questions/13575090/construct-pandas-dataframe-from-items-in-nested-dictionary\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_data = pd.DataFrame.from_dict({(i): data_dict[i]\n",
    "                                  for i in data_dict.keys()},orient='index')\n",
    "\n",
    "        \n",
    "##List of Features that are numbers\n",
    "numbers = list(df_data)  \n",
    "numbers.remove('email_address') \n",
    "numbers.remove('poi')\n",
    "\n",
    "##Ensure pd.dataframe values are 'float' for mathematical operations\n",
    "for trait in [numbers]:\n",
    "    df_data[trait] = df_data[trait].astype('float')\n",
    "\n",
    "## New features:\n",
    " \n",
    "df_data['take_home'] = df_data['salary'] + df_data['bonus']\n",
    "df_data['percent_exercised'] = df_data['exercised_stock_options']/df_data['total_stock_value']\n",
    "df_data['response_rate'] = df_data['from_messages']/df_data['to_messages']\n",
    "df_data['poi_response'] = df_data['from_this_person_to_poi']/df_data['from_poi_to_this_person']\n",
    "\n",
    "new_features=['take_home', 'percent_exercised', 'response_rate', 'poi_response']\n",
    "numbers = numbers + new_features \n",
    "\n",
    "\n",
    "##Data Prepocessing: Imputation (replace NaNs)\n",
    "##Replace inf with NaN\n",
    "#df_data.dropna(0,'all') -> no rows with all empty features\n",
    "df_data = df_data.replace(np.inf, np.nan)\n",
    "\n",
    "imputed_mean = df_data.copy()   \n",
    "imputed_median = df_data.copy()\n",
    "imputed_zero = df_data.copy()\n",
    "for col in numbers:\n",
    "    ave =  imputed_mean[col].mean()\n",
    "    imputed_mean[col] = imputed_mean[col].replace(np.nan, ave)\n",
    "    imputed_median[col] = imputed_median[col].replace(np.nan, ave)\n",
    "    imputed_zero[col] = imputed_zero[col].replace(np.nan, 0)\n",
    "\n",
    "#print imputed_mean.describe()\n",
    "#print imputed_mean.plot.box(rot=90)\n",
    "#print imputed_zero.describe()\n",
    "#print imputed_zero.plot.box(rot=90)\n",
    "\n",
    "## Removed features: \n",
    "## 'restricted_stock_deferred', 17 entries; 'loan_advances', 3 entries; 'director_fees',16 entries\n",
    "##  'deferral_payments, 38 entries (73% NaNs)\n",
    "##      -limited entries <10%, 2 have no POIs to classify on\n",
    "\n",
    "my_features = ['poi','salary', 'bonus','total_payments',  \n",
    "               'exercised_stock_options','restricted_stock','total_stock_value',\n",
    "               'expenses','deferred_income',\n",
    "               'long_term_incentive', 'other',\n",
    "               'to_messages', 'shared_receipt_with_poi','from_messages',      \n",
    "               'from_this_person_to_poi', 'from_poi_to_this_person',\n",
    "              # 'restricted_stock_deferred','loan_advances','director_fees', #Removed - too few points\n",
    "              # 'deferral_payments',\n",
    "              'take_home', 'percent_exercised', 'response_rate', 'poi_response' # New features\n",
    "              ]\n",
    "\n",
    "## Store new features and corrections to my_dataset dictionary for easy export below.\n",
    "my_data = df_data.to_dict(orient='index')\n",
    "my_data_median = imputed_median.to_dict(orient='index')\n",
    "my_data_mean = imputed_mean.to_dict(orient='index')\n",
    "my_data_zero = imputed_zero.to_dict(orient='index')\n",
    "my_dataset = imputed_zero.to_dict(orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.72954100e+06   1.72954100e+06   4.17500000e+06   2.01955000e+05\n",
      "   3.04805000e+05  -3.08105500e+06   1.52000000e+02   4.70000000e+01\n",
      "   6.50000000e+01   1.38297872e+00]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "def feature_extraction(mydata_dict, features_lineup):\n",
    "    data = featureFormat(mydata_dict, features_lineup, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    return labels,features\n",
    "\n",
    "### Split sets for cross validation train/test data \n",
    "def split_sets(features, labels, test_amt, r_state): \n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    f_train, f_test, l_train, l_test = \\\n",
    "    train_test_split(features, labels, test_size=test_amt, random_state=r_state)\n",
    "    return f_train, f_test, l_train, l_test   \n",
    "\n",
    "\n",
    "\n",
    "#labels, features = feature_extraction(my_dataset, my_features)\n",
    "#features_train, features_test, labels_train, labels_test = split_sets(features, labels, 0.3, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4.07485195e-16   1.29552474e-15  -2.67230484e-01  -6.07480676e-01\n",
      "  -4.81157870e-16  -5.22493919e-01  -1.34073716e+00   2.14970267e-16\n",
      "  -2.01659617e-15   2.10091579e-16   6.85134538e-16  -1.25142943e-15\n",
      "   6.40762271e-16  -9.20940043e-16  -4.23831393e-16   1.61667248e-15\n",
      "   1.30977200e+00  -3.79240754e-16  -5.06850054e-16]\n",
      "VarianceThreshold:\n",
      "9 other 1.0\n",
      "2 total_payments 1.0\n",
      "6 expenses 1.0\n",
      "15 take_home 1.0\n",
      "3 exercised_stock_options 1.0\n",
      "10 to_messages 1.0\n",
      "17 response_rate 1.0\n",
      "13 from_this_person_to_poi 1.0\n",
      "11 shared_receipt_with_poi 1.0\n",
      "14 from_poi_to_this_person 1.0\n",
      "18 poi_response 1.0\n",
      "4 restricted_stock 1.0\n",
      "1 bonus 1.0\n",
      "8 long_term_incentive 1.0\n",
      "5 total_stock_value 1.0\n",
      "16 percent_exercised 1.0\n",
      "0 salary 1.0\n",
      "7 deferred_income 1.0\n",
      "12 from_messages 1.0\n",
      "########  SelectKBest:\n",
      "percent_exercised score:  0.00130552398188 0.971228181585\n",
      "to_messages score:  0.366007900033 0.54616080033\n",
      "expenses score:  0.400610688055 0.527799224419\n",
      "from_messages score:  0.582370663056 0.446659512623\n",
      "poi_response score:  0.649517614987 0.421642317838\n",
      "response_rate score:  0.718297155008 0.39813937095\n",
      "from_this_person_to_poi score:  1.35263023793 0.246783341952\n",
      "other score:  1.85661693931 0.175188160159\n",
      "from_poi_to_this_person score:  3.01795985473 0.0845306715988\n",
      "shared_receipt_with_poi score:  5.69635156929 0.0183272757698\n",
      "deferred_income score:  5.79623973729 0.0173538298478\n",
      "long_term_incentive score:  5.92840697988 0.0161480837499\n",
      "restricted_stock score:  6.50152563671 0.0118466779559\n",
      "total_payments score:  8.20431885008 0.00481829670194\n",
      "salary score:  9.34243739009 0.00267903762018\n",
      "bonus score:  11.3688476593 0.00096408754967\n",
      "take_home score:  12.1424194194 0.000657095563698\n",
      "total_stock_value score:  19.7515310677 1.77152905642e-05\n",
      "exercised_stock_options score:  26.0304920143 1.06640442442e-06\n",
      "##########  SelectPercentile:\n",
      "percent_exercised score:  0.00212300044887 0.971228181585\n",
      "to_messages score:  0.0439846015133 0.54616080033\n",
      "expenses score:  0.0464714709731 0.527799224419\n",
      "from_messages score:  0.0586099861181 0.446659512623\n",
      "poi_response score:  0.0628015581958 0.421642317838\n",
      "response_rate score:  0.0669724792483 0.39813937095\n",
      "from_this_person_to_poi score:  0.101754222324 0.246783341952\n",
      "other score:  0.126672029489 0.175188160159\n",
      "from_poi_to_this_person score:  0.179667055493 0.0845306715988\n",
      "shared_receipt_with_poi score:  0.290837136978 0.0183272757698\n",
      "deferred_income score:  0.294806036838 0.0173538298478\n",
      "long_term_incentive score:  0.300042797217 0.0161480837499\n",
      "restricted_stock score:  0.322568358531 0.0118466779559\n",
      "total_payments score:  0.387989982129 0.00481829670194\n",
      "salary score:  0.430674408675 0.00267903762018\n",
      "bonus score:  0.504997338355 0.00096408754967\n",
      "take_home score:  0.532875061576 0.000657095563698\n",
      "total_stock_value score:  0.79564460995 1.77152905642e-05\n",
      "exercised_stock_options score:  1.0 1.06640442442e-06\n"
     ]
    }
   ],
   "source": [
    "#### from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler \n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "\n",
    "labels, features_raw = feature_extraction(my_data, my_features)\n",
    "\n",
    "imp_mean = Imputer(missing_values=np.nan, strategy='mean')\n",
    "features= imp_mean.fit_transform(features_raw)\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "print features[1]\n",
    "\n",
    "## Rank Features with various Feature Selection Methods\n",
    "######################\n",
    "## VarianceThreshold object to rank feature variances\n",
    "thresholder = VarianceThreshold()\n",
    "high_variance = thresholder.fit(features)\n",
    "## List Features with Ranked variances (ascending)\n",
    "t_vars = thresholder.variances_\n",
    "t_vars_sort = np.argsort(thresholder.variances_)\n",
    "print \"VarianceThreshold:\"\n",
    "for i in t_vars_sort:\n",
    "    print i, my_features[i+1], t_vars[i]\n",
    "\n",
    "######################\n",
    "## KBest takes Featues, target(labels), select k features\n",
    "kbest = SelectKBest(f_regression).fit(features, labels)\n",
    "k_scoresort = np.argsort(kbest.scores_)\n",
    "k_pvals  = kbest.pvalues_\n",
    "print \"########  SelectKBest:\"\n",
    "for f in k_scoresort:\n",
    "    print my_features[f+1], 'score: ', kbest.scores_[f], k_pvals[f]\n",
    "\n",
    "#####################    \n",
    "## Select Percentile, default selection function: the 10% most significant features\n",
    "selector = SelectPercentile(f_classif, percentile=10)\n",
    "selector.fit(features, labels)\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "scores /= scores.max()\n",
    "print \"##########  SelectPercentile:\"\n",
    "for f in np.argsort(scores):\n",
    "    print my_features[f+1], 'score: ', scores[f], k_pvals[f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('take_home', 'bonus') 0.993993530943\n",
      "('bonus', 'take_home') 0.993993530943\n",
      "('total_payments', 'loan_advances') 0.972798552984\n",
      "('loan_advances', 'total_payments') 0.972798552984\n",
      "('total_stock_value', 'exercised_stock_options') 0.964537075968\n",
      "('exercised_stock_options', 'total_stock_value') 0.964537075968\n",
      "('response_rate', 'from_messages') 0.927433275593\n",
      "('from_messages', 'response_rate') 0.927433275593\n",
      "('shared_receipt_with_poi', 'to_messages') 0.881262542007\n",
      "('to_messages', 'shared_receipt_with_poi') 0.881262542007\n",
      "('total_payments', 'other') 0.83829180367\n",
      "('other', 'total_payments') 0.83829180367\n"
     ]
    }
   ],
   "source": [
    "## Review Features: correlation matrix pandas, boxplot, statistics\n",
    "## https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\n",
    "\n",
    "#print imputed_zero.describe()\n",
    "\n",
    "s= imputed_zero.corr()\n",
    "s_order = s.unstack().sort_values(ascending=False)\n",
    "\n",
    "for key, value in s_order.iteritems():\n",
    "    if value > 0.8 and value !=1:\n",
    "        print key, value\n",
    "        \n",
    "#imputed_zero[my_features].plot.box(rot=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  10\n"
     ]
    }
   ],
   "source": [
    "### USE top scores from Variance Threshold, SelectKBest/SelectPercentile for order, remove high correlation features\n",
    "## feature_list = ['poi','total_stock_value','take_home',\n",
    "                #'exercised_stock_options','bonus',\n",
    "                #'salary', 'deferred_income','long_term_incentive',\n",
    "                #'total_payments','restricted_stock','shared_receipt_with_poi',\n",
    "                # 'expenses','from_poi_to_this_person', 'other',\n",
    "                # 'poi_response, 'from_this_person_to_poi',\n",
    "                # 'to_messages', 'from_messages', 'response_rate', 'percent_exercised'\n",
    "#                ]\n",
    "\n",
    "## Ranked for Zero imputation\n",
    "my_features = ['poi', #'total_stock_value', 'take_home',\n",
    "               'exercised_stock_options','bonus',\n",
    "               'salary', 'deferred_income','long_term_incentive',\n",
    "               #'total_payments',\n",
    "               'restricted_stock', 'shared_receipt_with_poi',\n",
    "               'expenses', 'from_poi_to_this_person', 'other', \n",
    "               #'poi_response', 'from_this_person_to_poi',\n",
    "               #\n",
    "              ]\n",
    "\n",
    "labels, features = feature_extraction(my_dataset, my_features)\n",
    "#labels, features = feature_extraction(my_data_imputed, my_features)\n",
    "print \"Features: \", len(my_features)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  10\n"
     ]
    }
   ],
   "source": [
    "### Ranked for Mean/Median\n",
    "my_features = ['poi', 'exercised_stock_options', 'total_stock_value', \n",
    "               'bonus', #'take_home', \n",
    "               'salary', #'total_payments',\n",
    "               'long_term_incentive',#'restricted_stock',\n",
    "               'deferred_income','other',#'shared_receipt_with_poi',\n",
    "               'from_poi_to_this_person','from_this_person_to_poi',  \n",
    "               #'response_rate' ,\n",
    "               'poi_response', \n",
    "               #'expenses','from_messages', 'to_messages', 'percent_exercised'\n",
    "              ]\n",
    "labels, features = feature_extraction(my_dataset, my_features)\n",
    "#labels, features = feature_extraction(my_data_imputed, my_features)\n",
    "print \"Features: \", len(my_features)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "If no scoring is specified, the estimator passed should have a 'score' method. The estimator Pipeline(steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('features', FeatureUnion(n_jobs=1,\n       transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n  svd_solver='auto', tol=0.0, whiten=False)), ('select', SelectKBest(k=10, score_func=<function f_classif at 0x10a386668>))],\n       transformer_weights=None))]) does not.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-ccec4c494121>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mmean_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/wendy/.local/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/wendy/.local/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/wendy/.local/lib/python2.7/site-packages/sklearn/metrics/scorer.pyc\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    271\u001b[0m         raise TypeError(\n\u001b[1;32m    272\u001b[0m             \u001b[0;34m\"If no scoring is specified, the estimator passed should \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \"have a 'score' method. The estimator %r does not.\" % estimator)\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: If no scoring is specified, the estimator passed should have a 'score' method. The estimator Pipeline(steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('features', FeatureUnion(n_jobs=1,\n       transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n  svd_solver='auto', tol=0.0, whiten=False)), ('select', SelectKBest(k=10, score_func=<function f_classif at 0x10a386668>))],\n       transformer_weights=None))]) does not."
     ]
    }
   ],
   "source": [
    "###########################\n",
    "## Pipeline for feature reduction with PCA()/SelectKBest\n",
    "## http://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html\n",
    "#sphx-glr-auto-examples-compose-plot-compare-reduction-py\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "labels, features = feature_extraction(my_data_median, my_features)\n",
    "sss = StratifiedShuffleSplit(labels, 50, test_size = 0.3, random_state = 42)\n",
    "\n",
    "\n",
    "## Combine PCA and Univariate Selection:\n",
    "pca = PCA()\n",
    "selectk = SelectKBest()\n",
    "united_features = FeatureUnion([(\"pca\", pca),(\"select\", selectk)])\n",
    "\n",
    "                  \n",
    "pipe = Pipeline([#(\"impute\",Imputer(strategy = 'mean')),\n",
    "                 (\"scale\", StandardScaler()),\n",
    "                 (\"features\", united_features)\n",
    "#                 ('reduce_dim', SelectKBest()),\n",
    "#                 ('classify', GaussianNB())\n",
    "])\n",
    "\n",
    "K_FEATURES_OPTIONS = [1, 5, 9, 10]\n",
    "S_FUNC = [f_regression, f_classif]\n",
    "C_OPTIONS = [25]\n",
    "                  \n",
    "param_grid = [{'features__pca__n_components': [1,3,5,7,10],\n",
    "               'features__pca__whiten': [True, False],\n",
    "               'features__select__k': K_FEATURES_OPTIONS,\n",
    "               'features__select__score_func': S_FUNC,\n",
    "               #'reduce_dim': [PCA()],\n",
    "               #'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "               'classify__C': C_OPTIONS\n",
    "              },\n",
    "              \n",
    "]\n",
    "                  \n",
    "#  {'reduce_dim': [SelectKBest()],\n",
    "#        'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "#        'reduce_dim__score_func': S_FUNC,\n",
    "#        'classify__C': C_OPTIONS\n",
    "#   },\n",
    "                  \n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=sss)\n",
    "grid.fit(features, labels)\n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print \"Pipeline Feature Reductions PCA()/SelectKBest with SCV classifier\"\n",
    "print mean_scores\n",
    "print grid.best_score_, grid.best_params_\n",
    "grid.cv_results_['params'][grid.best_index_]\n",
    "\n",
    "clf1 = grid.best_estimator_\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = split_sets(features, labels, 0.3, 42)\n",
    "clf1.fit(features_train,labels_train)\n",
    "\n",
    "print(\"Tester Scores:\")\n",
    "test_classifier(clf1, my_data_median, my_features, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print my_features\n",
    "print labels\n",
    "print features[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline with various classifiers\n",
      "[ 0.88418605  0.88837209  0.8944186   0.86837209  0.81209302  0.83069767\n",
      "  0.8455814   0.85116279  0.87162791  0.87348837  0.88372093  0.84790698\n",
      "  0.84744186  0.84744186  0.84976744  0.83813953  0.83162791]\n",
      "0.894418604651 {'classify__C': 25, 'classify': SVC(C=25, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)}\n",
      "{'classify__C': 25, 'classify': SVC(C=25, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)}\n",
      "Tester Scores:\n",
      "Pipeline(steps=[('scale', MinMaxScaler(copy=True, feature_range=(0, 1))), ('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('select', SelectKBest(k=1, score_func=<function f_regression at 0x10a3867d0>)), ('pca', PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,\n",
      "  svd_solver...,\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "\tAccuracy: 0.89205\tPrecision: 0.69757\tRecall: 0.12640\tF1: 0.21402\tF2: 0.15115\n",
      "\tTotal predictions: 43000\tTrue positives:  632\tFalse positives:  274\tFalse negatives: 4368\tTrue negatives: 37726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Pipeline for classifier selection with one feature reduction\n",
    "from sklearn import tree, neighbors\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "labels, features = feature_extraction(my_data_median, my_features)\n",
    "sss = StratifiedShuffleSplit(labels, 50, test_size = 0.3, random_state = 42)\n",
    "## Combine PCA and Univariate Selection:\n",
    "\n",
    "pca = PCA(n_components = 1, whiten=False)\n",
    "selectk = SelectKBest(k=1, score_func = f_regression)\n",
    "united_features = FeatureUnion([(\"select\", selectk),(\"pca\", pca)])\n",
    "\n",
    "pipe2 = Pipeline([(\"scale\", MinMaxScaler()),\n",
    "                 (\"features\", united_features),\n",
    "                 ('classify', SVC())\n",
    "])\n",
    "\n",
    "#pipe2 = Pipeline([(\"scale\", MinMaxScaler(feature_range=(0, 1))),\n",
    "#                  ('reduce_dim', PCA(n_components=3)),\n",
    "#                  ('reduce_dim', SelectKBest(k=1)),\n",
    "#                  ('classify', GaussianNB())\n",
    "#])\n",
    "\n",
    "#N_FEATURES_OPTIONS = [1,2,4]\n",
    "C_OPTIONS = [1, 5, 25]             #SVC\n",
    "SPLITS = [2, 10, 20]               #Decision Tree\n",
    "WEIGHTS = ['distance', 'uniform']  #K Nearest Neighbors\n",
    "NACHBARN = [5, 15]                 #K Nearest Neighbors\n",
    "ESTIMATES = [25, 50, 75]           #Ada Boost\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'classify': [SVC()],\n",
    "        'classify__C': C_OPTIONS \n",
    "    },\n",
    "    {\n",
    "        'classify': [GaussianNB()]\n",
    "    },\n",
    "    {\n",
    "        'classify': [tree.DecisionTreeClassifier()],\n",
    "        'classify__min_samples_split': SPLITS \n",
    "    },\n",
    "    {\n",
    "        'classify': [neighbors.KNeighborsClassifier()],\n",
    "        'classify__n_neighbors': NACHBARN,\n",
    "        'classify__weights': WEIGHTS\n",
    "    },\n",
    "    {\n",
    "        'classify': [RandomForestClassifier()],\n",
    "        'classify__n_estimators': ESTIMATES\n",
    "    },\n",
    "    {\n",
    "        'classify': [AdaBoostClassifier()],\n",
    "        'classify__n_estimators': ESTIMATES\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe2, param_grid=param_grid, cv= sss)\n",
    "grid.fit(features, labels)\n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "\n",
    "print \"Pipeline with various classifiers\"\n",
    "print mean_scores\n",
    "print grid.best_score_, grid.best_params_\n",
    "print grid.cv_results_['params'][grid.best_index_]\n",
    "\n",
    "clf2 = grid.best_estimator_\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = split_sets(features, labels, 0.3, 42)\n",
    "clf2.fit(features_train,labels_train)\n",
    "\n",
    "print(\"Tester Scores:\")\n",
    "test_classifier(clf2, my_data_median, my_features, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "##http://scikit-learn.org/0.16/auto_examples/model_selection/grid_search_digits.html#example-model-selection-grid-search-digits-py\n",
    "## \n",
    "\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import recall_score, accuracy_score, precision_score\n",
    "\n",
    "## Load/Prepare dataset\n",
    "labels, features = feature_extraction(my_data_median, my_features)\n",
    "sss = StratifiedShuffleSplit(labels, 50, test_size = 0.3, random_state = 42)\n",
    "\n",
    "\n",
    "pipe3 = Pipeline([#('scale', MinMaxScaler(feature_range=(0, 1))),\n",
    "#                  ('reduce_dim', PCA(n_components=3)),\n",
    "                  ('reduce_dim', SelectKBest()),\n",
    "                  ('classify', SVC())\n",
    "])\n",
    "\n",
    "## Set tuning parameters for cross-validation\n",
    "cees = [5, 7, 10, 15] # C values\n",
    "kay = [2,4,6,7]\n",
    "\n",
    "tune_KNNparameters = [{'classify__n_neighbors': [25, 50, 75, 100],\n",
    "                       'classify__weights':['distance', 'uniform'],\n",
    "                       'classify__algorithm': ['auto'],\n",
    "                       'classify__leaf_size':[1,3,5],\n",
    "                       'classify__p':[1]\n",
    "                      }\n",
    "                     ]\n",
    "tune_SVCparameters = [{'reduce_dim__k': kay,\n",
    "                       'reduce_dim__score_func':[f_classif, f_regression],\n",
    "                       'classify__kernel': ['rbf'],\n",
    "                       'classify__C': cees,\n",
    "                       'classify__gamma': [1e-5, 1e-6, 'auto']\n",
    "                      },\n",
    "                      {'reduce_dim__k': kay,\n",
    "                       'reduce_dim__score_func':[f_classif, f_regression],\n",
    "                       'classify__kernel': ['linear'],\n",
    "                       'classify__C': cees},\n",
    "                      #{'classify__kernel': ['poly'],   \n",
    "                      # 'classify__C': cees, \n",
    "                      # 'classify__degree':[3, 4, 5],\n",
    "                      # 'classify__gamma': [1e-5, 1e-6, 'auto']\n",
    "                      #}\n",
    "                     ]\n",
    "\n",
    "tune_RFparameters = [{'n_estimators': [9,10,12,15],\n",
    "                      'criterion': ['gini','entropy'],\n",
    "                      'min_samples_split': [2,3,4,5],\n",
    "#                     'min_samples_leaf': [1,2,3],\n",
    "                      'max_features': ['auto', 1, 0.5]\n",
    "                     }\n",
    "                  ]\n",
    "\n",
    "tune_ADAparameters = [{#'reduce_dim__k': kay,\n",
    "                       'classify__n_estimators': [2,5,50,100],\n",
    "                       'classify__algorithm': ['SAMME','SAMME.R'],\n",
    "                       'classify__learning_rate': [1, 0.5, 0.25],\n",
    "                     }\n",
    "                  ]\n",
    "\n",
    "print(\"Parameter Tuning\\n\")\n",
    "\n",
    "gs = GridSearchCV(pipe3, tune_SVCparameters, cv=sss)\n",
    "gs.fit(features, labels)\n",
    "clf = gs.best_estimator_\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = split_sets(features, labels, 0.3, 42)\n",
    "clf.fit(features_train,labels_train)\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "print(gs.best_score_, gs.best_params_)\n",
    "print(\"\\nGrid scores:\")\n",
    "for params, mean_score, scores in gs.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean_score, scores.std() * 2, params))\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Tester Scores:\")\n",
    "test_classifier(clf, my_data_zero, my_features, 1000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "             \n",
    "print(\"Prediction with SKfold shuffle cross validations.\")\n",
    "skfold = KFold(n_splits=5, shuffle=True)\n",
    "for train_indices, test_indices in skfold.split(labels):\n",
    "    features_train = [features[i] for i in train_indices]\n",
    "    features_test = [features[i] for i in test_indices]\n",
    "    labels_train = [labels[i] for i in train_indices]\n",
    "    labels_test = [labels[i] for i in test_indices]\n",
    "\n",
    "    clf_prediction = clf.predict(features_test)\n",
    "    print \"accuracy score: \", \\\n",
    "        accuracy_score(labels_test, clf_prediction)\n",
    "    print \"precision_score: \", \\\n",
    "        precision_score(labels_test, clf_prediction)\n",
    "    print \"recall_score\", \\\n",
    "        recall_score(labels_test, clf_prediction)\n",
    "\n",
    "print(\"Prediction with Single Split 50/50 validation\")\n",
    "features_train, features_test, labels_train, labels_test = split_sets(features, labels, 0.3, 9)\n",
    "y_true, y_pred = labels_train, clf.predict(features_train)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()\n",
    "\n",
    "### Precision Recall and F1 Metrics\n",
    "def prec_recall(model, pred, labels_test):\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    y_true = labels_test\n",
    "    y_pred = pred\n",
    "    print model \n",
    "    print \"Precision:\", precision_score(y_true, y_pred)  \n",
    "    print \"Recall:\", recall_score(y_true, y_pred)  \n",
    "    print \"F1 Score:\", f1_score(y_true, y_pred)\n",
    "    \n",
    "    \n",
    "prec_recall(clf, y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\\n",
    "\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, test_size = 0.3, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )       \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "                print \"All predictions should take value 0 or 1.\"\n",
    "                print \"Evaluating performance for processed predictions:\"\n",
    "                break\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "        print \"Precision or recall may be undefined due to a lack of true positive predictions.\"\n",
    "        print (total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Make Classifier \n",
    "\n",
    "clf2 = Pipeline([(\"scale\", MinMaxScaler(feature_range=(0, 1))),\n",
    "                 ('reduce_dim', PCA(n_components=3)),\n",
    "                 ('classify', neighbors.KNeighborsClassifier(p= 1, n_neighbors=35, \n",
    "                                                              algorithm= 'auto', weights='distance',\n",
    "                                                              leaf_size=3))\n",
    "                ])\n",
    "test_classifier(clf2, my_data_zero, my_features, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([(\"scale\", MinMaxScaler(feature_range=(0, 1))),\n",
    "                 #('reduce_dim', PCA(n_components=1)),\n",
    "                 ('classify', neighbors.KNeighborsClassifier(p= 1, n_neighbors=40, \n",
    "                                                              algorithm= 'auto', weights='distance',\n",
    "                                                              leaf_size=1))])\n",
    "test_classifier(clf, my_data_median, my_features, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
