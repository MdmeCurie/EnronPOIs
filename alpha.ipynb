{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHALEY DAVID A 18\n",
      "WROBEL BRUCE 18\n",
      "LOCKHART EUGENE E 20\n",
      "THE TRAVEL AGENCY IN THE PARK 18\n",
      "GRAMM WENDY L 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wendy/.local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# %load poi_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\". # You will need to use more features\n",
    "##### 'email_address' fails featureFormat() as it is string not float, all other features imported for \n",
    "##### initial testing and investigations\n",
    "features_list = ['poi', 'salary', 'bonus','deferral_payments','total_payments',  \n",
    "                 'exercised_stock_options','restricted_stock','total_stock_value',\n",
    "                 'expenses','deferred_income',\n",
    "                 'long_term_incentive', 'other',\n",
    "                 'restricted_stock_deferred', 'loan_advances', 'director_fees', \n",
    "                 'to_messages', 'shared_receipt_with_poi','from_messages',      \n",
    "                 'from_this_person_to_poi', 'from_poi_to_this_person'             \n",
    "                ] \n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "    \n",
    "for namen in data_dict:\n",
    "    count = 0\n",
    "    for things, values in data_dict[namen].items():\n",
    "        if values == 'NaN':\n",
    "            count +=1\n",
    "    if count >=18:\n",
    "        print namen, count\n",
    "        \n",
    "### Task 2: Remove outliers - remove TOTAL, Lockhardt Eugene E (all NaNs), and The Agency in the Park    \n",
    "outlier = data_dict.pop('TOTAL')\n",
    "outlier = data_dict.pop('LOCKHART EUGENE E')\n",
    "outlier = data_dict.pop('THE TRAVEL AGENCY IN THE PARK')\n",
    "\n",
    "### Negative outliers found in deferred_income and restricted stock deferred\n",
    "### Entries for 'BELFER ROBERT' & 'BHATANGAR SANJAY' corrected as confirmed by enron61702insiderpay.pdf\n",
    "\n",
    "data_dict['BELFER ROBERT']['deferred_income']  = -102500\n",
    "data_dict['BELFER ROBERT']['deferral_payments']= 'NaN'\n",
    "data_dict['BELFER ROBERT']['expenses'] = 3285\n",
    "data_dict['BELFER ROBERT']['director_fees'] = 102500 \n",
    "data_dict['BELFER ROBERT']['total_payments'] = 3285\n",
    "data_dict['BELFER ROBERT']['exercised_stock_options']= 'NaN'\n",
    "data_dict['BELFER ROBERT']['restricted_stock']= 44093\n",
    "data_dict['BELFER ROBERT']['restricted_stock_deferred']= -44093\n",
    "data_dict['BELFER ROBERT']['total_stock_value'] = 'NaN'\n",
    "\n",
    "data_dict['BHATNAGAR SANJAY']['other']= 'NaN' \n",
    "data_dict['BHATNAGAR SANJAY']['expenses']= 137864\n",
    "data_dict['BHATNAGAR SANJAY']['director_fees']= 'NaN'\n",
    "data_dict['BHATNAGAR SANJAY']['total_payments']= 137864\n",
    "data_dict['BHATNAGAR SANJAY']['exercised_stock_options']= 15456290 \n",
    "data_dict['BHATNAGAR SANJAY']['restricted_stock']= 2604490\n",
    "data_dict['BHATNAGAR SANJAY']['restricted_stock_deferred']= -2604490\n",
    "data_dict['BHATNAGAR SANJAY']['total_stock_value']= 15456290\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "\n",
    "## create dataframe of dict with features as cols and names as index\n",
    "#https://stackoverflow.com/questions/13575090/construct-pandas-dataframe-from-items-in-nested-dictionary\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_data = pd.DataFrame.from_dict({(i): data_dict[i]\n",
    "                                  for i in data_dict.keys()},orient='index')\n",
    "\n",
    "        \n",
    "##List of Features that are numbers\n",
    "numbers = list(df_data)  \n",
    "numbers.remove('email_address') \n",
    "numbers.remove('poi')\n",
    "\n",
    "##Ensure pd.dataframe values are 'float' for mathematical operations\n",
    "for trait in [numbers]:\n",
    "    df_data[trait] = df_data[trait].astype('float')\n",
    "\n",
    "## New features:\n",
    " \n",
    "df_data['take_home'] = df_data['salary'] + df_data['bonus']\n",
    "df_data['percent_exercised'] = df_data['exercised_stock_options']/df_data['total_stock_value']\n",
    "df_data['response_rate'] = df_data['from_messages']/df_data['to_messages']\n",
    "df_data['poi_response'] = df_data['from_this_person_to_poi']/df_data['from_poi_to_this_person']\n",
    "\n",
    "new_features=['take_home', 'percent_exercised', 'response_rate', 'poi_response']\n",
    "numbers = numbers + new_features \n",
    "\n",
    "\n",
    "##Data Prepocessing: Imputation (replace NaNs)\n",
    "##Replace inf with NaN\n",
    "#df_data.dropna(0,'all') -> no rows with all empty features\n",
    "df_data = df_data.replace(np.inf, np.nan)\n",
    "\n",
    "imputed_mean = df_data.copy()   \n",
    "imputed_median = df_data.copy()\n",
    "imputed_zero = df_data.copy()\n",
    "for col in numbers:\n",
    "    ave =  imputed_mean[col].mean()\n",
    "    imputed_mean[col] = imputed_mean[col].replace(np.nan, ave)\n",
    "    imputed_median[col] = imputed_median[col].replace(np.nan, ave)\n",
    "    imputed_zero[col] = imputed_zero[col].replace(np.nan, 0)\n",
    "\n",
    "#print imputed_mean.describe()\n",
    "#print imputed_mean.plot.box(rot=90)\n",
    "#print imputed_zero.describe()\n",
    "#print imputed_zero.plot.box(rot=90)\n",
    "\n",
    "## Scale in pd.dataframe MinMaxScaler-doesn't ignore strings, type for all features\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler()\n",
    "#scaled_data = scaler.fit_transform(scaled_data)\n",
    "\n",
    "#scaled_mean = imputed_mean.copy()\n",
    "#scaled_mean = ((imputed_mean - imputed_mean.min())/(imputed_mean.max()-imputed_mean.min())).astype('float')\n",
    "#scaled_median = imputed_mean.copy()\n",
    "#scaled_median = ((imputed_median - imputed_median.min())/(imputed_median.max()-imputed_median.min())).astype('float')\n",
    "#scaled_zero = imputed_zero.copy()\n",
    "#scaled_zero = ((imputed_zero - imputed_zero.min())/(imputed_zero.max()-imputed_zero.min())).astype('float')\n",
    "\n",
    "## Removed features: \n",
    "## 'restricted_stock_deferred', 17 entries; 'loan_advances', 3 entries; 'director_fees',16 entries\n",
    "##  'deferral_payments, 38 entries (73% NaNs)\n",
    "##      -limited entries <10%, 2 have no POIs to classify on\n",
    "my_features = ['poi','salary', 'bonus','total_payments',  \n",
    "               'exercised_stock_options','restricted_stock','total_stock_value',\n",
    "               'expenses','deferred_income',\n",
    "               'long_term_incentive', 'other',\n",
    "               'to_messages', 'shared_receipt_with_poi','from_messages',      \n",
    "               'from_this_person_to_poi', 'from_poi_to_this_person',\n",
    "              # 'restricted_stock_deferred','loan_advances','director_fees', #Removed - too few points\n",
    "              # 'deferral_payments',\n",
    "              'take_home', 'percent_exercised', 'response_rate', 'poi_response' # New features\n",
    "              ]\n",
    "\n",
    "## Store new features and corrections to my_dataset dictionary for easy export below.\n",
    "my_data_median = imputed_median.to_dict(orient='index')\n",
    "my_data_mean = imputed_mean.to_dict(orient='index')\n",
    "my_data_zero = imputed_zero.to_dict(orient='index')\n",
    "my_dataset = scaled_zero.to_dict(orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped =df_data.groupby(['poi']).agg({'salary':'count', 'bonus':'count','total_stock_value':'count',\n",
    "                                       'take_home':'count', 'deferred_income':'count',\n",
    "                                       'long_term_incentive':'count'})\n",
    "\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Separate Labels,Features\n",
    "### Split sets for train/test data \n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "def feature_extraction(mydata_dict, features_lineup):\n",
    "    data = featureFormat(mydata_dict, features_lineup, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    return labels,features\n",
    "\n",
    "def split_sets(features, labels, test_amt, r_state): \n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    f_train, f_test, l_train, l_test = \\\n",
    "    train_test_split(features, labels, test_size=test_amt, random_state=r_state)\n",
    "    return f_train, f_test, l_train, l_test   \n",
    "\n",
    "\n",
    "\n",
    "#labels, features = feature_extraction(my_dataset, my_features)\n",
    "#features_train, features_test, labels_train, labels_test = split_sets(features, labels, 0.3, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "labels, features = feature_extraction(my_data_zero, my_features)\n",
    "## Rank Features with various Feature Selection Methods\n",
    "## Create VarianceThreshold object to rank feature variances\n",
    "thresholder = VarianceThreshold()\n",
    "high_variance = thresholder.fit(features)\n",
    "## List Features with Ranked variances (ascending)\n",
    "t_vars = thresholder.variances_\n",
    "t_vars_sort = np.argsort(thresholder.variances_)\n",
    "print \"VarianceThreshold:\"\n",
    "for i in t_vars_sort:\n",
    "    print i, my_features[i+1], t_vars[i]\n",
    "\n",
    "######################\n",
    "## KBest takes Featues, target(labels), select k features\n",
    "kbest = SelectKBest(f_regression).fit(features, labels)\n",
    "k_scoresort = np.argsort(kbest.scores_)\n",
    "k_pvals  = kbest.pvalues_\n",
    "print \"########  SelectKBest:\"\n",
    "for f in k_scoresort:\n",
    "    print my_features[f+1], 'score: ', kbest.scores_[f], k_pvals[f]\n",
    "\n",
    "#####################    \n",
    "## Select Percentile, default selection function: the 10% most significant features\n",
    "selector = SelectPercentile(f_classif, percentile=10)\n",
    "selector.fit(features, labels)\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "scores /= scores.max()\n",
    "print \"##########  SelectPercentile:\"\n",
    "for f in np.argsort(scores):\n",
    "    print my_features[f+1], 'score: ', scores[f], k_pvals[f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Review Features: correlation matrix pandas, boxplot, statistics\n",
    "## https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\n",
    "\n",
    "#scaled_zero.describe()\n",
    "\n",
    "s= scaled_zero.corr()\n",
    "s_order = s.unstack().sort_values(ascending=False)\n",
    "\n",
    "for key, value in s_order.iteritems():\n",
    "    if value > 0.8 and value !=1:\n",
    "        print key, value\n",
    "        \n",
    "scaled_zero[my_features].plot.box(rot=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## my_features top scores from Variance Threshold, SelectKBest/SelectPercentile Order, remove high correlation\n",
    "#my_features = ['poi','exercised_stock_options', 'total_stock_value',\n",
    "#               'salary','bonus', 'total_payments',\n",
    "#               'restricted_stock','long_term_incentive',\n",
    "#               'deferred_income','shared_receipt_with_poi',\n",
    "#               'from_poi_to_this_person',\n",
    "#               'other', 'from_this_person_to_poi', 'from_messages', 'expenses'\n",
    "#                ]\n",
    "\n",
    "my_features = ['poi', 'total_stock_value',#'take_home',\n",
    "               'exercised_stock_options','bonus',\n",
    "               #'salary', 'deferred_income','long_term_incentive',\n",
    "               #'total_payments','restricted_stock','shared_receipt_with_poi',\n",
    "               # 'expenses','from_poi_to_this_person', 'other', \n",
    "              ]\n",
    "\n",
    "labels, features = feature_extraction(my_dataset, my_features)\n",
    "#labels, features = feature_extraction(my_data_imputed, my_features)\n",
    "print \"Features: \", len(my_features)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "## Pipeline for feature reduction with PCA()/SelectKBest\n",
    "## http://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html\n",
    "#sphx-glr-auto-examples-compose-plot-compare-reduction-py\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "labels, features = feature_extraction(my_data_zero, my_features)\n",
    "sss = StratifiedShuffleSplit(labels, 100, test_size = 0.3, random_state = 42)\n",
    "\n",
    "pipe = Pipeline([(\"scale\", MinMaxScaler(feature_range=(0, 1))),\n",
    "                 ('reduce_dim', SelectKBest()),\n",
    "                 ('classify', SVC())\n",
    "])\n",
    "\n",
    "N_FEATURES_OPTIONS = [1, 3, 5]\n",
    "S_FUNC = [f_regression, f_classif]\n",
    "C_OPTIONS = [5, 10, 25]\n",
    "param_grid = [\n",
    "    {\n",
    "        'reduce_dim': [SelectKBest()],\n",
    "        'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "        'reduce_dim__score_func': S_FUNC,\n",
    "        'classify__C': C_OPTIONS\n",
    "    },\n",
    "    {\n",
    "        'reduce_dim': [PCA()],\n",
    "        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "        'classify__C': C_OPTIONS\n",
    "    }, \n",
    "\n",
    "]\n",
    "## Function: test_classifier(clf, dataset, feature_list, folds)\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=sss)\n",
    "grid.fit(features, labels)\n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "\n",
    "print \"Pipeline Feature Reductions PCA()/SelectKBest with SCV classifier\"\n",
    "print mean_scores\n",
    "print grid.best_score_\n",
    "grid.cv_results_['params'][grid.best_index_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print my_features\n",
    "print labels\n",
    "print features[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Pipeline for classifier selection with one feature reduction\n",
    "from sklearn import tree, neighbors\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "labels, features = feature_extraction(my_data_zero, my_features)\n",
    "sss = StratifiedShuffleSplit(labels, 500, test_size = 0.3, random_state = 42)\n",
    "\n",
    "pipe2 = Pipeline([(\"scale\", MinMaxScaler(feature_range=(0, 1))),\n",
    "                  ('reduce_dim', PCA(n_components=3)),\n",
    "                  ('classify', GaussianNB())\n",
    "])\n",
    "\n",
    "N_FEATURES_OPTIONS = [1]\n",
    "C_OPTIONS = [5, 10, 25, 50]\n",
    "SPLITS = [2, 10, 20]\n",
    "WEIGHTS = ['distance', 'uniform']\n",
    "NACHBARN = [5, 10, 50]\n",
    "ESTIMATES = [2,10,25,50]\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "     #  'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "    #    'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "        'classify': [SVC()],\n",
    "        'classify__C': C_OPTIONS \n",
    "    },\n",
    "    {\n",
    "     #   'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "    #    'reduce_dim__n_components': N_FEATURES_OPTIONS, \n",
    "        'classify': [GaussianNB()]\n",
    "    },\n",
    "    {\n",
    "     #   'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "    #    'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "        'classify': [tree.DecisionTreeClassifier()],\n",
    "        'classify__min_samples_split': SPLITS \n",
    "    },\n",
    "    {\n",
    "     #  'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "    #    'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "        'classify': [neighbors.KNeighborsClassifier()],\n",
    "        'classify__n_neighbors': NACHBARN,\n",
    "        'classify__weights': WEIGHTS\n",
    "    },\n",
    "    {\n",
    "     #   'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "    #    'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "        'classify': [RandomForestClassifier()],\n",
    "        'classify__n_estimators': ESTIMATES\n",
    "    },\n",
    "    {\n",
    "    #    'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "    #    'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "        'classify': [AdaBoostClassifier()],\n",
    "        'classify__n_estimators': ESTIMATES\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipe2, param_grid=param_grid, cv= sss)\n",
    "grid.fit(features, labels)\n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "\n",
    "print \"Pipeline with various classifiers\"\n",
    "print mean_scores\n",
    "print grid.best_score_\n",
    "grid.cv_results_['params'][grid.best_index_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "##http://scikit-learn.org/0.16/auto_examples/model_selection/grid_search_digits.html#example-model-selection-grid-search-digits-py\n",
    "## \n",
    "\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import recall_score, accuracy_score, precision_score\n",
    "\n",
    "## Load/Prepare dataset\n",
    "labels, features = feature_extraction(my_data_zero, my_features)\n",
    "sss = StratifiedShuffleSplit(labels, 50, test_size = 0.4, random_state = 9)\n",
    "\n",
    "\n",
    "pipe3 = Pipeline([('scale', MinMaxScaler(feature_range=(0, 1))),\n",
    "                  ('reduce_dim', PCA(n_components=3)),\n",
    "#                  ('reduce_dim', SelectKBest(k=3, score_func = f_regression)),\n",
    "                  ('classify', neighbors.KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "## Set tuning parameters for cross-validation\n",
    "cees = [1, 3, 5, 10, 25] # C values\n",
    "kay = [1,3,6]\n",
    "\n",
    "tune_KNNparameters = [{'classify__n_neighbors': [35, 40, 45],\n",
    "                       'classify__weights':['distance'],\n",
    "                       'classify__algorithm': ['auto'],\n",
    "                       'classify__leaf_size':[1,3,5],\n",
    "                       'classify__p':[1]\n",
    "                      }\n",
    "                     ]\n",
    "tune_SVCparameters = [{'classify__kernel': ['rbf'],\n",
    "                       'classify__C': cees,\n",
    "                       'classify__gamma': [1e-5, 1e-6, 'auto']\n",
    "                      },\n",
    "                      {'classify__kernel': ['linear'],\n",
    "                       'classify__C': cees},\n",
    "                      {'classify__kernel': ['poly'],   \n",
    "                       'classify__C': cees, \n",
    "                       'classify__degree':[3, 4, 5],\n",
    "                       'classify__gamma': [1e-5, 1e-6, 'auto']\n",
    "                      }\n",
    "                     ]\n",
    "\n",
    "tune_RFparameters = [{'n_estimators': [9,10,12,15],\n",
    "                      'criterion': ['gini','entropy'],\n",
    "                      'min_samples_split': [2,3,4,5],\n",
    "#                     'min_samples_leaf': [1,2,3],\n",
    "                      'max_features': ['auto', 1, 0.5]\n",
    "                     }\n",
    "                  ]\n",
    "\n",
    "tune_ADAparameters = [{#'reduce_dim__k': kay,\n",
    "                       'classify__n_estimators': [2,5,50,100],\n",
    "                       'classify__algorithm': ['SAMME','SAMME.R'],\n",
    "                       'classify__learning_rate': [1, 0.5, 0.25],\n",
    "                     }\n",
    "                  ]\n",
    "\n",
    "print(\"Parameter Tuning\\n\")\n",
    "\n",
    "gs = GridSearchCV(pipe3, tune_KNNparameters, cv=sss)\n",
    "gs.fit(features, labels)\n",
    "clf = gs.best_estimator_\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = split_sets(features, labels, 0.3, 42)\n",
    "clf.fit(features_train,labels_train)\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "print(gs.best_params_, gs.best_score_)\n",
    "print(\"\\nGrid scores:\")\n",
    "for params, mean_score, scores in gs.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean_score, scores.std() * 2, params))\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"Tester Scores:\")\n",
    "test_classifier(clf, my_data_zero, my_features, 1000)\n",
    "\n",
    "             \n",
    "print(\"Prediction with SKfold shuffle cross validations.\")\n",
    "skfold = KFold(n_splits=5, shuffle=True)\n",
    "for train_indices, test_indices in skfold.split(labels):\n",
    "    features_train = [features[i] for i in train_indices]\n",
    "    features_test = [features[i] for i in test_indices]\n",
    "    labels_train = [labels[i] for i in train_indices]\n",
    "    labels_test = [labels[i] for i in test_indices]\n",
    "\n",
    "    clf_prediction = clf.predict(features_test)\n",
    "    print \"accuracy score: \", \\\n",
    "        accuracy_score(labels_test, clf_prediction)\n",
    "    print \"precision_score: \", \\\n",
    "        precision_score(labels_test, clf_prediction)\n",
    "    print \"recall_score\", \\\n",
    "        recall_score(labels_test, clf_prediction)\n",
    "    print \"PRED \",clf_prediction\n",
    "    print \"TRUE \",labels_test\n",
    "\n",
    "print(\"Prediction with Single Split 50/50 validation\")\n",
    "features_train, features_test, labels_train, labels_test = split_sets(features, labels, 0.3, 9)\n",
    "y_true, y_pred = labels_train, clf.predict(features_train)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print()\n",
    "print \"PREDy \",y_pred\n",
    "print \"TRUEy \",y_true\n",
    "\n",
    "### Precision Recall and F1 Metrics\n",
    "def prec_recall(model, pred, labels_test):\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    y_true = labels_test\n",
    "    y_pred = pred\n",
    "    print model \n",
    "    print \"Precision:\", precision_score(y_true, y_pred)  \n",
    "    print \"Recall:\", recall_score(y_true, y_pred)  \n",
    "    print \"F1 Score:\", f1_score(y_true, y_pred)\n",
    "    \n",
    "    \n",
    "prec_recall(clf, y_pred, y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\\n",
    "\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list, folds):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, test_size = 0.3, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )       \n",
    "        ### fit the classifier using training set, and test on test set\n",
    "        clf.fit(features_train, labels_train)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "                print \"All predictions should take value 0 or 1.\"\n",
    "                print \"Evaluating performance for processed predictions:\"\n",
    "                break\n",
    "    try:\n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "        print \"Precision or recall may be undefined due to a lack of true positive predictions.\"\n",
    "        print (total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Make Classifier \n",
    "\n",
    "clf2 = Pipeline([(\"scale\", MinMaxScaler(feature_range=(0, 1))),\n",
    "                 ('reduce_dim', PCA(n_components=3)),\n",
    "                 ('classify', neighbors.KNeighborsClassifier(p= 1, n_neighbors=35, \n",
    "                                                              algorithm= 'auto', weights='distance',\n",
    "                                                              leaf_size=3))\n",
    "                ])\n",
    "test_classifier(clf2, my_data_zero, my_features, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([(\"scale\", MinMaxScaler(feature_range=(0, 1))),\n",
    "                 #('reduce_dim', PCA(n_components=1)),\n",
    "                 ('classify', neighbors.KNeighborsClassifier(p= 1, n_neighbors=40, \n",
    "                                                              algorithm= 'auto', weights='distance',\n",
    "                                                              leaf_size=1))])\n",
    "test_classifier(clf, my_data_median, my_features, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
