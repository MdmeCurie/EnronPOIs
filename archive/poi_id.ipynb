{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHALEY DAVID A 18\n",
      "WROBEL BRUCE 18\n",
      "LOCKHART EUGENE E 20\n",
      "THE TRAVEL AGENCY IN THE PARK 18\n",
      "GRAMM WENDY L 18\n",
      "('Shape of dataframe:', (143, 26))\n",
      "########  VarianceThreshold:\n",
      "17 percent_exercised 0.0428556892348\n",
      "18 response_rate 0.0894548270954\n",
      "19 poi_response 8.41993327813\n",
      "14 from_poi_to_this_person 4711.15340603\n",
      "20 delta_response 5874.1310822\n",
      "13 from_this_person_to_poi 6217.49630789\n",
      "11 shared_receipt_with_poi 870854.226588\n",
      "12 from_messages 2091963.78747\n",
      "10 to_messages 4143367.05003\n",
      "6 expenses 1463705686.85\n",
      "0 salary 20549708177.6\n",
      "7 deferred_income 334378201613.0\n",
      "8 long_term_incentive 359340069307.0\n",
      "15 deferral_payments 502195602396.0\n",
      "1 bonus 1.21288736787e+12\n",
      "9 other 1.27161273531e+12\n",
      "16 take_home 1.38554742061e+12\n",
      "4 restricted_stock 3.84187314364e+12\n",
      "3 exercised_stock_options 2.28487079716e+13\n",
      "5 total_stock_value 3.85540599423e+13\n",
      "2 total_payments 7.65245705268e+13\n",
      "########  SelectKBest:\n",
      "percent_exercised score:  0.000401940670368 0.984033059035\n",
      "poi_response score:  0.0154671467977 0.901201838337\n",
      "from_messages score:  0.189798310011 0.663750746968\n",
      "delta_response score:  0.20690160034 0.649906374025\n",
      "response_rate score:  0.245418351257 0.621090983482\n",
      "deferral_payments score:  0.287728742886 0.592524976736\n",
      "to_messages score:  0.866375799185 0.353551520815\n",
      "expenses score:  0.885612140348 0.348278640555\n",
      "from_this_person_to_poi score:  2.18707096885 0.141404334743\n",
      "other score:  3.98829256858 0.0477421671689\n",
      "from_poi_to_this_person score:  4.22581832359 0.041658411587\n",
      "shared_receipt_with_poi score:  7.38569122485 0.00739930814679\n",
      "restricted_stock score:  8.03474082344 0.00526344622287\n",
      "long_term_incentive score:  8.39079462433 0.00437336860671\n",
      "total_payments score:  8.82689237649 0.00349058796307\n",
      "deferred_income score:  10.3373895574 0.00161646280003\n",
      "salary score:  10.8815010433 0.00122970676706\n",
      "bonus score:  15.8234101956 0.000110736954439\n",
      "take_home score:  16.7873635215 7.02073395781e-05\n",
      "total_stock_value score:  21.7975215598 6.98445269619e-06\n",
      "exercised_stock_options score:  24.404148638 2.18000301618e-06\n",
      "##########  SelectPercentile:\n",
      "percent_exercised score:  0.00123470070277 0.984033059035\n",
      "poi_response score:  0.0079797913388 0.901201838337\n",
      "from_messages score:  0.031439305528 0.663750746968\n",
      "delta_response score:  0.0330562192579 0.649906374025\n",
      "response_rate score:  0.036535053944 0.621090983482\n",
      "deferral_payments score:  0.0401468896178 0.592524976736\n",
      "to_messages score:  0.079756931603 0.353551520815\n",
      "expenses score:  0.080909597798 0.348278640555\n",
      "from_this_person_to_poi score:  0.150054020827 0.141404334743\n",
      "other score:  0.23334590816 0.0477421671689\n",
      "from_poi_to_this_person score:  0.243802319708 0.041658411587\n",
      "shared_receipt_with_poi score:  0.37636540454 0.00739930814679\n",
      "restricted_stock score:  0.402492721147 0.00526344622287\n",
      "long_term_incentive score:  0.41670335388 0.00437336860671\n",
      "total_payments score:  0.433998550267 0.00349058796307\n",
      "deferred_income score:  0.493051864487 0.00161646280003\n",
      "salary score:  0.514029211755 0.00122970676706\n",
      "bonus score:  0.698697773898 0.000110736954439\n",
      "take_home score:  0.733654686193 7.02073395781e-05\n",
      "total_stock_value score:  0.910682424509 6.98445269619e-06\n",
      "exercised_stock_options score:  1.0 2.18000301618e-06\n",
      "('take_home', 'bonus') 0.995309445937\n",
      "('bonus', 'take_home') 0.995309445937\n",
      "('exercised_stock_options', 'total_stock_value') 0.938867717086\n",
      "('total_stock_value', 'exercised_stock_options') 0.938867717086\n",
      "('from_messages', 'response_rate') 0.926513951173\n",
      "('response_rate', 'from_messages') 0.926513951173\n",
      "('to_messages', 'shared_receipt_with_poi') 0.847990014598\n",
      "('shared_receipt_with_poi', 'to_messages') 0.847990014598\n",
      "('total_payments', 'other') 0.826444807901\n",
      "('other', 'total_payments') 0.826444807901\n",
      "Features:  11\n",
      "Pipeline Feature Union PCA()/SelectKBest Best Score/Parameters:\n",
      "0.865581395349\n",
      "{'features__pca__n_components': 3, 'features__select__k': 2, 'features__pca__whiten': True, 'features__select__score_func': <function chi2 at 0x1146f87d0>}\n",
      "<<<Tester Results>>>\n",
      "Pipeline(steps=[('impute', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scale', MinMaxScaler(copy=True, feature_range=(0, 1))), ('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=True)), ('select', SelectKBest(k=2, score_func=<function chi2 at 0x1146f87d0>))],\n",
      "       transformer_weights=None)), ('classify', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.85540\tPrecision: 0.45415\tRecall: 0.41850\tF1: 0.43560\tF2: 0.42518\n",
      "\tTotal predictions: 15000\tTrue positives:  837\tFalse positives: 1006\tFalse negatives: 1163\tTrue negatives: 11994\n",
      "\n",
      "Pipeline Scores of Various Classifiers\n",
      "[ 0.87581395  0.85906977  0.80046512  0.85767442  0.85395349  0.85627907\n",
      "  0.88372093  0.88372093  0.86651163  0.86418605  0.83534884  0.83116279]\n",
      "BEST ESTIMATOR SCORE: 0.883720930233\n",
      "BEST PARAMETERS: {'classify__weights': 'distance', 'classify': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=30, p=2,\n",
      "           weights='distance'), 'classify__n_neighbors': 30}\n",
      "\n",
      "<<<Tester Results Classifier Review>>>\n",
      "Got a divide by zero when trying out: Pipeline(steps=[('impute', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scale', MinMaxScaler(copy=True, feature_range=(0, 1))), ('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
      " ...ski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=30, p=2,\n",
      "           weights='distance'))])\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n",
      "Parameter Tuning\n",
      "\n",
      "('Best score:', 0.86356589147286822)\n",
      "Best parameters:\n",
      "{'classify__priors': None}\n",
      "<<<Tester Scores>>>\n",
      "Pipeline(steps=[('impute', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('scale', MinMaxScaler(copy=True, feature_range=(0, 1))), ('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('pca', PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=True)), ('select', SelectKBest(k=2, score_func=<function chi2 at 0x1146f87d0>))],\n",
      "       transformer_weights=None)), ('classify', GaussianNB(priors=None))])\n",
      "\tAccuracy: 0.85540\tPrecision: 0.45415\tRecall: 0.41850\tF1: 0.43560\tF2: 0.42518\n",
      "\tTotal predictions: 15000\tTrue positives:  837\tFalse positives: 1006\tFalse negatives: 1163\tTrue negatives: 11994\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %load poi_id.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data, test_classifier\n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler \n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree, neighbors\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import recall_score, accuracy_score, precision_score\n",
    "\n",
    "### Functions for Data Extraction and train_test_split ###\n",
    "###\n",
    "### Extract features and labels from dataset for local testing\n",
    "def feature_extraction(mydata_dict, features_lineup):\n",
    "    data = featureFormat(mydata_dict, features_lineup, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    return labels,features\n",
    "\n",
    "### Split sets for cross validation train/test data \n",
    "def split_sets(features, labels, test_amt, r_state): \n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    f_train, f_test, l_train, l_test = \\\n",
    "    train_test_split(features, labels, test_size=test_amt, random_state=r_state)\n",
    "    return f_train, f_test, l_train, l_test  \n",
    "###\n",
    "###########################################################\n",
    "\n",
    "### Task 1: Select Features to Use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\". \n",
    "##### 'email_address' fails featureFormat() as it is string not float, all other features imported \n",
    "\n",
    "features_list = ['poi', 'salary', 'bonus','deferral_payments','total_payments',  \n",
    "                 'exercised_stock_options','restricted_stock','total_stock_value',\n",
    "                 'expenses','deferred_income',\n",
    "                 'long_term_incentive', 'other',\n",
    "                 'restricted_stock_deferred', 'loan_advances', 'director_fees', \n",
    "                 'to_messages', 'shared_receipt_with_poi','from_messages',      \n",
    "                 'from_this_person_to_poi', 'from_poi_to_this_person'             \n",
    "                ] \n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "print (\"Rows with >=18 empty values:\")    \n",
    "for namen in data_dict:\n",
    "    count = 0\n",
    "    for things, values in data_dict[namen].items():\n",
    "        if values == 'NaN':\n",
    "            count +=1\n",
    "    if count >=18:\n",
    "        print namen, count\n",
    "        \n",
    "### Task 2: Remove outliers - remove TOTAL, Lockhardt Eugene E (all NaNs), and The Agency in the Park    \n",
    "outlier = data_dict.pop('TOTAL')\n",
    "outlier = data_dict.pop('LOCKHART EUGENE E')\n",
    "outlier = data_dict.pop('THE TRAVEL AGENCY IN THE PARK')\n",
    "\n",
    "### Negative outliers found in deferred_income and restricted stock deferred\n",
    "### Entries for 'BELFER ROBERT' & 'BHATANGAR SANJAY' corrected as confirmed by enron61702insiderpay.pdf\n",
    "\n",
    "data_dict['BELFER ROBERT']['deferred_income']  = -102500\n",
    "data_dict['BELFER ROBERT']['deferral_payments']= 'NaN'\n",
    "data_dict['BELFER ROBERT']['expenses'] = 3285\n",
    "data_dict['BELFER ROBERT']['director_fees'] = 102500 \n",
    "data_dict['BELFER ROBERT']['total_payments'] = 3285\n",
    "data_dict['BELFER ROBERT']['exercised_stock_options']= 'NaN'\n",
    "data_dict['BELFER ROBERT']['restricted_stock']= 44093\n",
    "data_dict['BELFER ROBERT']['restricted_stock_deferred']= -44093\n",
    "data_dict['BELFER ROBERT']['total_stock_value'] = 'NaN'\n",
    "\n",
    "data_dict['BHATNAGAR SANJAY']['other']= 'NaN' \n",
    "data_dict['BHATNAGAR SANJAY']['expenses']= 137864\n",
    "data_dict['BHATNAGAR SANJAY']['director_fees']= 'NaN'\n",
    "data_dict['BHATNAGAR SANJAY']['total_payments']= 137864\n",
    "data_dict['BHATNAGAR SANJAY']['exercised_stock_options']= 15456290 \n",
    "data_dict['BHATNAGAR SANJAY']['restricted_stock']= 2604490\n",
    "data_dict['BHATNAGAR SANJAY']['restricted_stock_deferred']= -2604490\n",
    "data_dict['BHATNAGAR SANJAY']['total_stock_value']= 15456290\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### created dataframe from dict with features as cols and names as index\n",
    "### https://stackoverflow.com/questions/13575090/construct-pandas-dataframe-from-items-in-nested-dictionary\n",
    "\n",
    "df_data = pd.DataFrame.from_dict({(i): data_dict[i]\n",
    "                                  for i in data_dict.keys()},orient='index')\n",
    "\n",
    "### List of Features that are numbers\n",
    "numbers = list(df_data)  \n",
    "numbers.remove('email_address') \n",
    "numbers.remove('poi')\n",
    "\n",
    "### Ensure pd.dataframe values are 'float' for mathematical operations\n",
    "for trait in [numbers]:\n",
    "    df_data[trait] = df_data[trait].astype('float')\n",
    "\n",
    "### New features:\n",
    " \n",
    "df_data['take_home'] = df_data['salary'] + df_data['bonus']\n",
    "df_data['percent_exercised'] = df_data['exercised_stock_options']/df_data['total_stock_value']\n",
    "df_data['response_rate'] = df_data['from_messages']/df_data['to_messages']\n",
    "df_data['poi_response'] = df_data['from_this_person_to_poi']/df_data['from_poi_to_this_person']\n",
    "df_data['delta_response'] = df_data['from_this_person_to_poi']-df_data['from_poi_to_this_person']\n",
    "\n",
    "### Replace Inf in poi_response with .max() + 10 to put at top of scale\n",
    "m = df_data.loc[df_data['poi_response'] != np.inf, 'poi_response'].max() + 10\n",
    "df_data['poi_response'].replace(np.inf,m,inplace=True)\n",
    "\n",
    "new_features=['take_home', 'percent_exercised', 'response_rate', 'poi_response', 'delta_response']\n",
    "numbers = numbers + new_features \n",
    "\n",
    "df_data.dropna(0,'all')  ##Drop rows with all empty features - technically unnecessary as no rows completely empty\n",
    "\n",
    "print (\"Shape of new dataframe:\", df_data.shape)\n",
    "\n",
    "### Imputation Data Prepocessing (replace NaNs)\n",
    "\n",
    "### Manual Imputation\n",
    "imputed_mean = df_data.copy()   \n",
    "imputed_median = df_data.copy()\n",
    "imputed_zero = df_data.copy()\n",
    "for col in numbers:\n",
    "    ave =  imputed_mean[col].mean()\n",
    "    imputed_mean[col] = imputed_mean[col].replace(np.nan, ave)\n",
    "    imputed_median[col] = imputed_median[col].replace(np.nan, ave)\n",
    "    imputed_zero[col] = imputed_zero[col].replace(np.nan, 0)\n",
    "\n",
    "\n",
    "my_features = ['poi','salary', 'bonus','total_payments',  \n",
    "               'exercised_stock_options','restricted_stock','total_stock_value',\n",
    "               'expenses','deferred_income',\n",
    "               'long_term_incentive', 'other',\n",
    "               'to_messages', 'shared_receipt_with_poi','from_messages',      \n",
    "               'from_this_person_to_poi', 'from_poi_to_this_person',\n",
    "              # 'restricted_stock_deferred','loan_advances','director_fees',    ## Removed - too few values\n",
    "               'deferral_payments',                                            ## Remove? - 73% NaNs\n",
    "               'take_home', 'percent_exercised', 'response_rate', 'poi_response', 'delta_response' ## New features\n",
    "              ]\n",
    "\n",
    "### Store new features and corrections to my_dataset dictionary for easy export below.\n",
    "my_dataset = df_data.to_dict(orient='index')               ## no pre-imputation, for imputation by pipeline\n",
    "\n",
    "my_data_median = imputed_median.to_dict(orient='index')    ## median manual pre-imputation\n",
    "my_data_mean = imputed_mean.to_dict(orient='index')        ## mean manual pre-imputation\n",
    "my_data_zero = imputed_zero.to_dict(orient='index')        ## zero manual pre-imputation\n",
    "\n",
    "\n",
    "######################\n",
    "## Rank Features with various Feature Selection Methods\n",
    "######################\n",
    "\n",
    "labels, features_raw = feature_extraction(my_dataset, my_features)\n",
    "\n",
    "imp_mean = Imputer(missing_values=np.nan, strategy='median')\n",
    "features= imp_mean.fit_transform(features_raw)\n",
    "\n",
    "print (\"########  Feature Ranking  ########\")\n",
    "## VarianceThreshold object to rank feature variances\n",
    "thresholder = VarianceThreshold()\n",
    "high_variance = thresholder.fit(features)\n",
    "## List Features with Ranked variances (ascending)\n",
    "t_vars = thresholder.variances_\n",
    "t_vars_sort = np.argsort(thresholder.variances_)\n",
    "print \"########  VarianceThreshold:\"\n",
    "for i in t_vars_sort:\n",
    "    print i, my_features[i+1], t_vars[i]\n",
    "    \n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "######################\n",
    "## KBest takes Featues, target(labels), select k features\n",
    "kbest = SelectKBest(f_regression).fit(features, labels)\n",
    "k_scoresort = np.argsort(kbest.scores_)\n",
    "k_pvals  = kbest.pvalues_\n",
    "print \"########  SelectKBest:\"\n",
    "for f in k_scoresort:\n",
    "    print my_features[f+1], 'score: ', kbest.scores_[f], k_pvals[f]\n",
    "\n",
    "#####################    \n",
    "## Select Percentile, default selection function: the 10% most significant features\n",
    "selector = SelectPercentile(f_classif, percentile=10)\n",
    "selector.fit(features, labels)\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "scores /= scores.max()\n",
    "print \"##########  SelectPercentile:\"\n",
    "for f in np.argsort(scores):\n",
    "    print my_features[f+1], 'score: ', scores[f], k_pvals[f]\n",
    "    \n",
    "\n",
    "### Highly correlated features\n",
    "## Review Features: correlation matrix pandas, boxplot, statistics\n",
    "## https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\n",
    "\n",
    "#print imputed_median.describe()\n",
    "#pd.DataFrame.hist(imputed_median)\n",
    "\n",
    "s= imputed_median.corr()\n",
    "s_order = s.unstack().sort_values(ascending=False)\n",
    "\n",
    "for key, value in s_order.iteritems():\n",
    "    if value > 0.8 and value !=1:\n",
    "        print key, value\n",
    "\n",
    "### Top Features Sorted as Ranked for Median Imputation by Select KBest/Percentile\n",
    "my_features = ['poi', 'exercised_stock_options', 'total_stock_value', \n",
    "               #'bonus', \n",
    "               'take_home', \n",
    "               'salary', 'deferred_income',\n",
    "               'total_payments',\n",
    "               'long_term_incentive', 'restricted_stock',\n",
    "               'shared_receipt_with_poi',\n",
    "               'from_poi_to_this_person',\n",
    "               #'other',\n",
    "               'from_this_person_to_poi', #'expenses',\n",
    "               #'to_messages',\n",
    "               #'response_rate' , #'delta_response',\n",
    "               #'from_messages'#,  'poi_response', 'percent_exercised'\n",
    "              ]\n",
    "labels, features = feature_extraction(my_dataset, my_features)\n",
    "\n",
    "print \">>> Selected Features: \", len(my_features)-1, my_features\n",
    "\n",
    "###########################\n",
    "## Feature Union Pipeline for feature selection/reduction with PCA()/SelectKBest optimization\n",
    "## http://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html\n",
    "\n",
    "labels, features = feature_extraction(my_dataset, my_features)\n",
    "sss = StratifiedShuffleSplit(labels, 50, test_size = 0.3, random_state = 42)\n",
    "\n",
    "pca = PCA()\n",
    "selectk = SelectKBest()\n",
    "united_features = FeatureUnion([(\"pca\", pca),(\"select\", selectk)])\n",
    "\n",
    "pipe_features = Pipeline([(\"impute\",Imputer(strategy = 'median')),\n",
    "                          (\"scale\", MinMaxScaler()),\n",
    "                          (\"features\", united_features),\n",
    "                          ('classify', GaussianNB())\n",
    "                         ])\n",
    "\n",
    "## Grid Search Parameters for Feature Selection/Reduction\n",
    "K_FEATURES_OPTIONS = [1,2,3]\n",
    "N_COMPS = [3,4,5]       \n",
    "S_FUNC = [f_regression, f_classif, chi2]\n",
    "C_OPTIONS = [25]\n",
    "              \n",
    "param_grid_f = [{'features__pca__n_components': N_COMPS,\n",
    "                 'features__pca__whiten': [True, False],\n",
    "                 'features__select__k': K_FEATURES_OPTIONS,\n",
    "                 'features__select__score_func': S_FUNC\n",
    "                }]\n",
    "\n",
    "grid_feature = GridSearchCV(pipe_features, param_grid=param_grid_f, cv=sss)\n",
    "grid_feature.fit(features, labels)\n",
    "\n",
    "print \"Pipeline Feature Union PCA()/SelectKBest Best Score/Parameters:\"\n",
    "#mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "#print mean_scores\n",
    "print grid_feature.best_score_\n",
    "print grid_feature.best_params_\n",
    "\n",
    "## Test Feature Selection with Prediction Scores\n",
    "clf_feature_selection = grid_feature.best_estimator_\n",
    "\n",
    "print(\"<<<Tester Results>>>\")\n",
    "test_classifier(clf_feature_selection, my_dataset, my_features)\n",
    "\n",
    "\n",
    "############\n",
    "## Pipeline for Classifier Reviews: limited parameter tuning of different classifiers\n",
    "############\n",
    "\n",
    "labels, features = feature_extraction(my_dataset, my_features)\n",
    "sss = StratifiedShuffleSplit(labels, 50, test_size = 0.3, random_state = 42)\n",
    "\n",
    "## Combine PCA and Univariate Selection using Parameters Determined Previously in Feature Union ##\n",
    "pca = PCA(n_components = 3, whiten=True)\n",
    "selectk = SelectKBest(k=2, score_func = chi2)\n",
    "united_features = FeatureUnion([(\"pca\", pca),(\"select\", selectk)])\n",
    "\n",
    "pipe_cr = Pipeline([('impute',Imputer(strategy = 'median')),\n",
    "                    ('scale', MinMaxScaler()),\n",
    "                    ('features', united_features),\n",
    "                    ('classify', SVC())\n",
    "                   ])\n",
    "\n",
    "##Classifier Parameters\n",
    "C_OPTIONS = [1, 25]                #SVC\n",
    "SPLITS = [2, 30]                   #Decision Tree\n",
    "WEIGHTS = ['distance', 'uniform']  #K Nearest Neighbors\n",
    "NACHBARN = [5, 30]                 #K Nearest Neighbors\n",
    "ESTIMATES = [10, 50]               #Ada Boost/Random Forest\n",
    "\n",
    "param_grid_cr = [\n",
    "    {\n",
    "        'classify': [SVC()],\n",
    "        'classify__C': C_OPTIONS \n",
    "    },\n",
    "    #{ \n",
    "    #    'classify': [GaussianNB()]   #Used as default in Feature Union Determination\n",
    "    #},\n",
    "    {\n",
    "        'classify': [tree.DecisionTreeClassifier()],\n",
    "        'classify__min_samples_split': SPLITS \n",
    "    },\n",
    "    {\n",
    "        'classify': [neighbors.KNeighborsClassifier()],\n",
    "        'classify__n_neighbors': NACHBARN,\n",
    "        'classify__weights': WEIGHTS\n",
    "    },\n",
    "    {\n",
    "        'classify': [RandomForestClassifier()],\n",
    "        'classify__n_estimators': ESTIMATES\n",
    "    },\n",
    "    {\n",
    "        'classify': [AdaBoostClassifier()],\n",
    "        'classify__n_estimators': ESTIMATES\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipe_cr, param_grid=param_grid_cr, cv= sss)\n",
    "grid.fit(features, labels)\n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "\n",
    "print \"Pipeline Scores of Various Classifiers\"\n",
    "print mean_scores\n",
    "print \"BEST ESTIMATOR SCORE:\", grid.best_score_\n",
    "print \"BEST PARAMETERS:\", grid.best_params_\n",
    "\n",
    "clf_classifier_review = grid.best_estimator_\n",
    "\n",
    "print\n",
    "print(\"<<<Tester Results Classifier Review>>>\")\n",
    "test_classifier(clf_classifier_review, my_dataset, my_features)\n",
    "\n",
    "#########################################################\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "##http://scikit-learn.org/0.16/auto_examples/model_selection/grid_search_digits.html#example-model-selection-grid-search-digits-py\n",
    "## \n",
    "\n",
    "\n",
    "\n",
    "## Load/Prepare dataset\n",
    "labels, features = feature_extraction(my_dataset, my_features)\n",
    "sss = StratifiedShuffleSplit(labels, 75, test_size = 0.3, random_state = 42)\n",
    "\n",
    "pca = PCA(n_components = 3, whiten=True)\n",
    "selectk = SelectKBest(k = 2, score_func = chi2)\n",
    "united_features = FeatureUnion([(\"pca\", pca),(\"select\", selectk)])\n",
    "\n",
    "\n",
    "pipe3 = Pipeline([(\"impute\",Imputer(strategy = 'median')),\n",
    "                  (\"scale\", MinMaxScaler()),\n",
    "                  (\"features\", united_features),\n",
    "                  ('classify',GaussianNB())\n",
    "                  ])\n",
    "\n",
    "## Set tuning parameters for cross-validation\n",
    "tune_GNBparameters = [{'classify__priors': [None]}]\n",
    "\n",
    "tune_KNNparameters = [{'classify__n_neighbors': [11,14],\n",
    "                       'classify__weights':['uniform', 'distance'],\n",
    "                       'classify__algorithm': ['auto'],\n",
    "                       'classify__leaf_size':[5,8],\n",
    "                       'classify__p':[2,1]\n",
    "                      }]\n",
    "\n",
    "cees = [1,20,50] # C values\n",
    "tune_SVCparameters = [{'classify__kernel': ['rbf'],\n",
    "                       'classify__C': cees},\n",
    "                       #'classify__gamma': [1e-5, 1e-6, 'auto']},\n",
    "                      {'classify__kernel': ['linear'],\n",
    "                       'classify__C': cees},\n",
    "                      {'classify__kernel': ['poly'],   \n",
    "                       'classify__C': cees, \n",
    "                       'classify__degree':[3, 4, 5]}\n",
    "                       #'classify__gamma': [1e-5, 1e-6, 'auto']}\n",
    "                     ]\n",
    "\n",
    "tune_RFparameters = [{'classify__n_estimators': [5,10,15],\n",
    "                      'classify__criterion': ['gini','entropy'],\n",
    "                      'classify__min_samples_split': [2,3,5],\n",
    "#                     'clasify__min_samples_leaf': [1,2,3],\n",
    "                      'classify__max_features': ['auto', 1, 0.5]\n",
    "                     }]\n",
    "\n",
    "tune_ADAparameters = [{'features__select__k': [2,3],\n",
    "                       'classify__n_estimators': [12, 50],\n",
    "                       'classify__algorithm': ['SAMME.R', 'SAMME'],\n",
    "                       'classify__learning_rate': [1, 0.4]}\n",
    "                     ]\n",
    "\n",
    "print(\">>>>>Chosen Classifier GaussionNB()<<<<<\\n\")\n",
    "gs = GridSearchCV(pipe3, tune_GNBparameters, cv=sss)\n",
    "gs.fit(features, labels)\n",
    "clf = gs.best_estimator_\n",
    "print(\"Best score:\",gs.best_score_)\n",
    "print(\"Best parameters:\")\n",
    "print(gs.best_params_)\n",
    "\n",
    "print(\"<<<Tester Scores on Final Estimator>>>\")\n",
    "test_classifier(clf, my_dataset, my_features)\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "features_list = my_features\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Feature Union PCA()/SelectKBest Best Score/Parameters:\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This GridSearchCV instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-f0b18737da2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#mean_scores = np.array(grid.cv_results_['mean_test_score'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#print mean_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mgrid_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mgrid_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/wendy/.local/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36mbest_score_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbest_score_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cv_results_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_index_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/wendy/.local/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# FIXME NotFittedError_ --> NotFittedError in 0.19\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_NotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This GridSearchCV instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "###########################\n",
    "## Feature Union Pipeline for feature selection/reduction with PCA()/SelectKBest optimization\n",
    "## http://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html\n",
    "\n",
    "labels, features = feature_extraction(my_dataset, my_features)\n",
    "sss = StratifiedShuffleSplit(labels, 50, test_size = 0.3, random_state = 42)\n",
    "\n",
    "pca = PCA()\n",
    "selectk = SelectKBest()\n",
    "united_features = FeatureUnion([(\"pca\", pca),(\"select\", selectk)])\n",
    "\n",
    "pipe_features = Pipeline([(\"impute\",Imputer(strategy = 'median')),\n",
    "                          (\"scale\", MinMaxScaler()),\n",
    "                          (\"features\", united_features),\n",
    "                          ('classify', GaussianNB())\n",
    "                         ])\n",
    "\n",
    "## Grid Search Parameters for Feature Selection/Reduction\n",
    "K_FEATURES_OPTIONS = [1,2,3]\n",
    "N_COMPS = [3,4,5]       \n",
    "S_FUNC = [f_regression, f_classif, chi2]\n",
    "C_OPTIONS = [25]\n",
    "              \n",
    "param_grid_f = [{'features__pca__n_components': N_COMPS,\n",
    "                 'features__pca__whiten': [True, False],\n",
    "                 'features__select__k': K_FEATURES_OPTIONS,\n",
    "                 'features__select__score_func': S_FUNC\n",
    "                }]\n",
    "\n",
    "grid_feature = GridSearchCV(pipe_features, param_grid=param_grid_f, cv=sss)\n",
    "#grid_feature.fit(features, labels)\n",
    "\n",
    "print \"Pipeline Feature Union PCA()/SelectKBest Best Score/Parameters:\"\n",
    "#mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "#print mean_scores\n",
    "print grid_feature.best_score_\n",
    "print grid_feature.best_params_\n",
    "\n",
    "## Test Feature Selection with Prediction Scores\n",
    "clf_feature_selection = grid_feature.best_estimator_\n",
    "\n",
    "print(\"<<<Tester Results>>>\")\n",
    "test_classifier(clf_feature_selection, my_dataset, my_features)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
