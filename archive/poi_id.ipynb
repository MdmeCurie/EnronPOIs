{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load poi_id.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data, test_classifier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif, chi2\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler \n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree, neighbors\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import recall_score, accuracy_score, precision_score\n",
    "\n",
    "### Functions for Data Extraction and train_test_split ###\n",
    "###\n",
    "### Extract features and labels from dataset for local testing\n",
    "def feature_extraction(mydata_dict, features_lineup):\n",
    "    data = featureFormat(mydata_dict, features_lineup, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    return labels,features\n",
    "\n",
    "### Split sets for cross validation train/test data \n",
    "def split_sets(features, labels, test_amt, r_state): \n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    f_train, f_test, l_train, l_test =     train_test_split(features, labels, test_size=test_amt, random_state=r_state)\n",
    "    return f_train, f_test, l_train, l_test  \n",
    "###\n",
    "###########################################################\n",
    "\n",
    "### Task 1: Select Features to Use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\". \n",
    "##### 'email_address' fails featureFormat() as it is string not float, all other features imported \n",
    "\n",
    "features_list = ['poi', 'salary', 'bonus','deferral_payments','total_payments',  \n",
    "                 'exercised_stock_options','restricted_stock','total_stock_value',\n",
    "                 'expenses','deferred_income',\n",
    "                 'long_term_incentive', 'other',\n",
    "                 'restricted_stock_deferred', 'loan_advances', 'director_fees', \n",
    "                 'to_messages', 'shared_receipt_with_poi','from_messages',      \n",
    "                 'from_this_person_to_poi', 'from_poi_to_this_person'             \n",
    "                ] \n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "num_entries = len(data_dict)\n",
    "print (\"There are %s entries in the original data_file\"%(num_entries))\n",
    "\n",
    "\n",
    "        \n",
    "### Task 2: Remove outliers - remove TOTAL, Lockhardt Eugene E (all NaNs), and The Agency in the Park    \n",
    "\n",
    "print (\"Rows with excessive empty values >=18:\")    \n",
    "for namen in data_dict:\n",
    "    count = 0\n",
    "    for things, values in data_dict[namen].items():\n",
    "        if values == 'NaN':\n",
    "            count +=1\n",
    "    if count >=18:\n",
    "        print namen, count\n",
    "\n",
    "outlier = data_dict.pop('LOCKHART EUGENE E')\n",
    "outlier = data_dict.pop('THE TRAVEL AGENCY IN THE PARK')\n",
    "\n",
    "## Remove Total as Statistical outlier found abnormal hist/distribution, found upon examination as pandas dataframe\n",
    "outlier = data_dict.pop('TOTAL')                       \n",
    "\n",
    "\n",
    "### Negative outliers found in deferred_income and restricted stock deferred\n",
    "### Entries for 'BELFER ROBERT' & 'BHATANGAR SANJAY' corrected as confirmed by enron61702insiderpay.pdf\n",
    "\n",
    "data_dict['BELFER ROBERT']['deferred_income']  = -102500\n",
    "data_dict['BELFER ROBERT']['deferral_payments']= 'NaN'\n",
    "data_dict['BELFER ROBERT']['expenses'] = 3285\n",
    "data_dict['BELFER ROBERT']['director_fees'] = 102500 \n",
    "data_dict['BELFER ROBERT']['total_payments'] = 3285\n",
    "data_dict['BELFER ROBERT']['exercised_stock_options']= 'NaN'\n",
    "data_dict['BELFER ROBERT']['restricted_stock']= 44093\n",
    "data_dict['BELFER ROBERT']['restricted_stock_deferred']= -44093\n",
    "data_dict['BELFER ROBERT']['total_stock_value'] = 'NaN'\n",
    "\n",
    "data_dict['BHATNAGAR SANJAY']['other']= 'NaN' \n",
    "data_dict['BHATNAGAR SANJAY']['expenses']= 137864\n",
    "data_dict['BHATNAGAR SANJAY']['director_fees']= 'NaN'\n",
    "data_dict['BHATNAGAR SANJAY']['total_payments']= 137864\n",
    "data_dict['BHATNAGAR SANJAY']['exercised_stock_options']= 15456290 \n",
    "data_dict['BHATNAGAR SANJAY']['restricted_stock']= 2604490\n",
    "data_dict['BHATNAGAR SANJAY']['restricted_stock_deferred']= -2604490\n",
    "data_dict['BHATNAGAR SANJAY']['total_stock_value']= 15456290\n",
    "\n",
    "### Task 3: Create new feature(s)\n",
    "### created dataframe from dict with features as cols and names as index\n",
    "### https://stackoverflow.com/questions/13575090/construct-pandas-dataframe-from-items-in-nested-dictionary\n",
    "\n",
    "df_data = pd.DataFrame.from_dict({(i): data_dict[i]\n",
    "                                  for i in data_dict.keys()},orient='index')\n",
    "\n",
    "## Outliers values with statistically high values - TOTAL discovered - removed above\n",
    "#print df_data['bonus'].describe()\n",
    "#df_data['bonus'].hist()\n",
    "#df_data['salary'].hist()\n",
    "#print df_data[df_data['bonus'] >0.8e8]\n",
    "\n",
    "num_pois = df_data[df_data['poi']==True].sum()\n",
    "print(\"There are %s entries classified as POIs\"%(num_pois))\n",
    "### List of Features that are numbers\n",
    "numbers = list(df_data)  \n",
    "numbers.remove('email_address') \n",
    "numbers.remove('poi')\n",
    "\n",
    "### Ensure pd.dataframe values are 'float' for mathematical operations\n",
    "for trait in [numbers]:\n",
    "    df_data[trait] = df_data[trait].astype('float')\n",
    "\n",
    "### New features:\n",
    " \n",
    "df_data['take_home'] = df_data['salary'] + df_data['bonus']\n",
    "df_data['percent_exercised'] = df_data['exercised_stock_options']/df_data['total_stock_value']\n",
    "df_data['response_rate'] = df_data['from_messages']/df_data['to_messages']\n",
    "df_data['poi_response'] = df_data['from_this_person_to_poi']/df_data['from_poi_to_this_person']\n",
    "df_data['delta_response'] = df_data['from_this_person_to_poi']-df_data['from_poi_to_this_person']\n",
    "\n",
    "### Replace Inf in poi_response with .max() + 10 to put at top of scale\n",
    "m = df_data.loc[df_data['poi_response'] != np.inf, 'poi_response'].max() + 10\n",
    "df_data['poi_response'].replace(np.inf,m,inplace=True)\n",
    "\n",
    "new_features=['take_home', 'percent_exercised', 'response_rate', 'poi_response', 'delta_response']\n",
    "numbers = numbers + new_features \n",
    "\n",
    "df_data.dropna(0,'all')  ##Drop rows with all empty features - technically unnecessary as no rows completely empty\n",
    "\n",
    "print (\"Shape of new dataframe:\", df_data.shape)\n",
    "\n",
    "### Imputation Data Prepocessing (replace NaNs)\n",
    "\n",
    "### Manual Imputation\n",
    "imputed_mean = df_data.copy()   \n",
    "imputed_median = df_data.copy()\n",
    "imputed_zero = df_data.copy()\n",
    "for col in numbers:\n",
    "    ave =  imputed_mean[col].mean()\n",
    "    imputed_mean[col] = imputed_mean[col].replace(np.nan, ave)\n",
    "    imputed_median[col] = imputed_median[col].replace(np.nan, ave)\n",
    "    imputed_zero[col] = imputed_zero[col].replace(np.nan, 0)\n",
    "\n",
    "## How many entries are there per Feature??    \n",
    "print (\"Number of NaNs per Feature of 143 \")\n",
    "print df_data.isnull().sum(axis=0)  #https://stackoverflow.com/questions/30059260/python-pandas-counting-the-number-of-missing-nan-in-each-row\n",
    "\n",
    "## Feature values, numbers and negative values\n",
    "print (\"Breakdown of Feature Value Entries:\")\n",
    "from __future__ import division\n",
    "for items in features_list:\n",
    "    value_exists = 0\n",
    "    pos_poi = 0\n",
    "    neg_poi = 0 \n",
    "    for names in data_dict:\n",
    "        if data_dict[names][items] != \"NaN\":\n",
    "            value_exists = value_exists + 1\n",
    "            if data_dict[names]['poi'] == 1:\n",
    "                pos_poi = pos_poi+1\n",
    "            else:    neg_poi = neg_poi+1\n",
    "    print '%25s %8d entries %8d POIs %8d non-POIs %8.1f%% POIs'%(items, value_exists, pos_poi, neg_poi,(pos_poi/value_exists)*100.0)\n",
    "\n",
    "print (\"Features with Negative Values:\")    \n",
    "for items in features_list:\n",
    "    neg_value = 0 \n",
    "    for names in data_dict:\n",
    "        if data_dict[names][items] != \"NaN\" and data_dict[names][items] <0:\n",
    "            neg_value = neg_value + 1\n",
    "    if neg_value >0:\n",
    "        print '%25s %8d'%(items, neg_value)\n",
    "        \n",
    "\n",
    "my_features = ['poi','salary', 'bonus','total_payments',  \n",
    "               'exercised_stock_options','restricted_stock','total_stock_value',\n",
    "               'expenses','deferred_income',\n",
    "               'long_term_incentive', 'other',\n",
    "               'to_messages', 'shared_receipt_with_poi','from_messages',      \n",
    "               'from_this_person_to_poi', 'from_poi_to_this_person',\n",
    "              # 'restricted_stock_deferred','loan_advances','director_fees',    ## Remove! - too few values\n",
    "               'deferral_payments',                                            ## Remove? - 73% NaNs\n",
    "               'take_home', 'percent_exercised', 'response_rate', 'poi_response', 'delta_response' ## New features\n",
    "              ]\n",
    "\n",
    "### Store new features and corrections to my_dataset dictionary for easy export below.\n",
    "my_dataset = df_data.to_dict(orient='index')               ## no pre-imputation, use imputation in estimatore pipeline\n",
    "\n",
    "\n",
    "######################\n",
    "## Rank Features with various Feature Selection Methods\n",
    "######################\n",
    "\n",
    "labels, features_raw = feature_extraction(my_dataset, my_features)\n",
    "\n",
    "imp_mean = Imputer(missing_values=np.nan, strategy='median')\n",
    "features= imp_mean.fit_transform(features_raw)\n",
    "\n",
    "print (\"########  Feature Ranking  ########\")\n",
    "## VarianceThreshold object to rank feature variances\n",
    "thresholder = VarianceThreshold()\n",
    "high_variance = thresholder.fit(features)\n",
    "## List Features with Ranked variances (descending)\n",
    "t_vars = thresholder.variances_\n",
    "#t_vars_sort = np.argsort(thresholder.variances_) ## ascending\n",
    "t_vars_sort = (-thresholder.variances_).argsort()##https://stackoverflow.com/questions/16486252/is-it-possible-to-use-argsort-in-descending-order\n",
    "\n",
    "print \"########  VarianceThreshold:\"\n",
    "for i in t_vars_sort:\n",
    "    print '%23s   %10.2e'%(my_features[i+1],t_vars[i])\n",
    "\n",
    "######################\n",
    "## SelectKBest Ranking Featues, target(labels), select k features\n",
    "kbest = SelectKBest(f_regression).fit(features, labels)\n",
    "print \"########  SelectKBest:\"\n",
    "for f in (-kbest.scores_).argsort():\n",
    "    print '%23s   %8.2e %10.2e'%(my_features[f+1], kbest.scores_[f], kbest.pvalues_[f])\n",
    "    \n",
    "#####################   Removed - Identical Scores to SelectKBest \n",
    "## Select Percentile, default selection function: the 10% most significant features\n",
    "#selectp = SelectPercentile(f_classif, percentile=10)\n",
    "#selectp.fit(features, labels)\n",
    "#print \"##########  SelectPercentile:\"\n",
    "#for f in (-scores).argsort():\n",
    "#    print '%23s   %8.2e %10.2e'%(my_features[f+1], selectp.scores_[f], selectp.pvalues_[f])\n",
    "  \n",
    "\n",
    "#####################      \n",
    "## Feature Importance with Extra Trees Classifier\n",
    "## https://machinelearningmastery.com/feature-selection-machine-learning-python/\n",
    "model = ExtraTreesClassifier(n_estimators = 1000).fit(features,labels)\n",
    "feature_scores = model.feature_importances_\n",
    "\n",
    "names = list(my_features)\n",
    "names.pop(0)\n",
    "for score, fname in sorted(zip(feature_scores, names), reverse=True):\n",
    "     print '%23s   %8.3f'%(fname, score)\n",
    "\n",
    "### Highly correlated features\n",
    "## Review Features: correlation matrix pandas, boxplot, statistics\n",
    "## https://stackoverflow.com/questions/17778394/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\n",
    "\n",
    "#print imputed_median.describe()\n",
    "#pd.DataFrame.hist(imputed_median)\n",
    "\n",
    "s= df_data.corr()\n",
    "s_order = s.unstack().sort_values(ascending=False)\n",
    "dup= 0\n",
    "eliminated = ['poi','restricted_stock_deferred','loan_advances','director_fees']\n",
    "for key, value in s_order.iteritems():\n",
    "    if key[0] in eliminated or key[1] in eliminated:\n",
    "        continue\n",
    "    if dup == value:\n",
    "        continue\n",
    "    if value > 0.8 and value !=1:\n",
    "        print key, value\n",
    "        dup = value\n",
    "\n",
    "### Top Features Sorted as Ranked for Median Imputation by Select KBest/Percentile\n",
    "my_features = ['poi', 'exercised_stock_options', 'total_stock_value', \n",
    "               #'bonus', \n",
    "               'take_home', \n",
    "               'salary', 'deferred_income',\n",
    "               'total_payments',\n",
    "               'long_term_incentive', 'restricted_stock',\n",
    "               'shared_receipt_with_poi',\n",
    "               'from_poi_to_this_person',\n",
    "               #'other',\n",
    "               'from_this_person_to_poi', #'expenses',\n",
    "               #'to_messages',\n",
    "               #'response_rate' , #'delta_response',\n",
    "               #'from_messages'#,  'poi_response', 'percent_exercised'\n",
    "              ]\n",
    "labels, features = feature_extraction(my_dataset, my_features)\n",
    "\n",
    "print \">>> Selected Features: \", len(my_features)-1, my_features\n",
    "\n",
    "###########################\n",
    "## Feature Union Pipeline for feature selection/reduction with PCA()/SelectKBest optimization\n",
    "## http://scikit-learn.org/stable/auto_examples/compose/plot_compare_reduction.html\n",
    "\n",
    "labels, features = feature_extraction(my_dataset, my_features)\n",
    "sss = StratifiedShuffleSplit(labels, 50, test_size = 0.3, random_state = 42)\n",
    "\n",
    "pca = PCA()\n",
    "selectk = SelectKBest()\n",
    "united_features = FeatureUnion([(\"pca\", pca),(\"select\", selectk)])\n",
    "\n",
    "pipe_features = Pipeline([(\"impute\",Imputer(strategy = 'median')),\n",
    "                          (\"scale\", MinMaxScaler()),\n",
    "                          (\"features\", united_features),\n",
    "                          ('classify', GaussianNB())\n",
    "                         ])\n",
    "\n",
    "## Grid Search Parameters for Feature Selection/Reduction\n",
    "K_FEATURES_OPTIONS = [1,2,3]\n",
    "N_COMPS = [3,4,5]       \n",
    "S_FUNC = [f_regression, f_classif, chi2]\n",
    "              \n",
    "param_grid_f = [{'features__pca__n_components': N_COMPS,\n",
    "                 'features__pca__whiten': [True, False],\n",
    "                 'features__select__k': K_FEATURES_OPTIONS,\n",
    "                 'features__select__score_func': S_FUNC\n",
    "                }]\n",
    "\n",
    "grid_feature = GridSearchCV(pipe_features, param_grid=param_grid_f, cv=sss)\n",
    "grid_feature.fit(features, labels)\n",
    "\n",
    "print \"Pipeline Feature Union PCA()/SelectKBest Best Score/Parameters:\"\n",
    "#mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "#print mean_scores\n",
    "print grid_feature.best_score_\n",
    "print grid_feature.best_params_\n",
    "\n",
    "## Test Feature Selection with Prediction Scores\n",
    "clf_feature_selection = grid_feature.best_estimator_\n",
    "\n",
    "print(\"<<<Tester Results>>>\")\n",
    "test_classifier(clf_feature_selection, my_dataset, my_features)\n",
    "\n",
    "\n",
    "############\n",
    "## Pipeline for Classifier Reviews: limited parameter tuning of different classifiers\n",
    "############\n",
    "\n",
    "labels, features = feature_extraction(my_dataset, my_features)\n",
    "sss = StratifiedShuffleSplit(labels, 50, test_size = 0.3, random_state = 42)\n",
    "\n",
    "## Combine PCA and Univariate Selection using Parameters Determined Previously in Feature Union ##\n",
    "pca = PCA(n_components = 3, whiten=True)\n",
    "selectk = SelectKBest(k=2, score_func = chi2)\n",
    "united_features = FeatureUnion([(\"pca\", pca),(\"select\", selectk)])\n",
    "\n",
    "pipe_cr = Pipeline([('impute',Imputer(strategy = 'median')),\n",
    "                    ('scale', MinMaxScaler()),\n",
    "                    ('features', united_features),\n",
    "                    ('classify', SVC())\n",
    "                   ])\n",
    "\n",
    "##Classifier Parameters\n",
    "C_OPTIONS = [1, 25]                #SVC\n",
    "SPLITS = [2, 30]                   #Decision Tree\n",
    "WEIGHTS = ['distance', 'uniform']  #K Nearest Neighbors\n",
    "NACHBARN = [5, 30]                 #K Nearest Neighbors\n",
    "ESTIMATES = [10, 50]               #Ada Boost/Random Forest\n",
    "\n",
    "param_grid_cr = [\n",
    "    {\n",
    "        'classify': [SVC()],\n",
    "        'classify__C': C_OPTIONS \n",
    "    },\n",
    "    #{ \n",
    "    #    'classify': [GaussianNB()]   #Used as default in Feature Union Determination\n",
    "    #},\n",
    "    {\n",
    "        'classify': [tree.DecisionTreeClassifier()],\n",
    "        'classify__min_samples_split': SPLITS \n",
    "    },\n",
    "    {\n",
    "        'classify': [neighbors.KNeighborsClassifier()],\n",
    "        'classify__n_neighbors': NACHBARN,\n",
    "        'classify__weights': WEIGHTS\n",
    "    },\n",
    "    {\n",
    "        'classify': [RandomForestClassifier()],\n",
    "        'classify__n_estimators': ESTIMATES\n",
    "    },\n",
    "    {\n",
    "        'classify': [AdaBoostClassifier()],\n",
    "        'classify__n_estimators': ESTIMATES\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipe_cr, param_grid=param_grid_cr, cv= sss)\n",
    "grid.fit(features, labels)\n",
    "\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "\n",
    "print \"Pipeline Scores of Various Classifiers\"\n",
    "print mean_scores\n",
    "print \"BEST ESTIMATOR SCORE:\", grid.best_score_\n",
    "print \"BEST PARAMETERS:\", grid.best_params_\n",
    "\n",
    "clf_classifier_review = grid.best_estimator_\n",
    "\n",
    "print\n",
    "print(\"<<<Tester Results Classifier Review>>>\")\n",
    "test_classifier(clf_classifier_review, my_dataset, my_features)\n",
    "\n",
    "#########################################################\n",
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "##http://scikit-learn.org/0.16/auto_examples/model_selection/grid_search_digits.html#example-model-selection-grid-search-digits-py\n",
    "## \n",
    "\n",
    "\n",
    "\n",
    "## Load/Prepare dataset\n",
    "labels, features = feature_extraction(my_dataset, my_features)\n",
    "sss = StratifiedShuffleSplit(labels, 75, test_size = 0.3, random_state = 42)\n",
    "\n",
    "pca = PCA(n_components = 3, whiten=True)\n",
    "selectk = SelectKBest(k = 2, score_func = chi2)\n",
    "united_features = FeatureUnion([(\"pca\", pca),(\"select\", selectk)])\n",
    "\n",
    "\n",
    "pipe3 = Pipeline([(\"impute\",Imputer(strategy = 'median')),\n",
    "                  (\"scale\", MinMaxScaler()),\n",
    "                  (\"features\", united_features),\n",
    "                  ('classify',GaussianNB())\n",
    "                  ])\n",
    "\n",
    "## Set tuning parameters for cross-validation\n",
    "tune_GNBparameters = [{'classify__priors': [None]}]\n",
    "\n",
    "tune_KNNparameters = [{'classify__n_neighbors': [11,14],\n",
    "                       'classify__weights':['uniform', 'distance'],\n",
    "                       'classify__algorithm': ['auto'],\n",
    "                       'classify__leaf_size':[5,8],\n",
    "                       'classify__p':[2,1]\n",
    "                      }]\n",
    "\n",
    "cees = [1,20,50] # C values\n",
    "tune_SVCparameters = [{'classify__kernel': ['rbf'],\n",
    "                       'classify__C': cees},\n",
    "                       #'classify__gamma': [1e-5, 1e-6, 'auto']},\n",
    "                      {'classify__kernel': ['linear'],\n",
    "                       'classify__C': cees},\n",
    "                      {'classify__kernel': ['poly'],   \n",
    "                       'classify__C': cees, \n",
    "                       'classify__degree':[3, 4, 5]}\n",
    "                       #'classify__gamma': [1e-5, 1e-6, 'auto']}\n",
    "                     ]\n",
    "\n",
    "tune_RFparameters = [{'classify__n_estimators': [5,10,15],\n",
    "                      'classify__criterion': ['gini','entropy'],\n",
    "                      'classify__min_samples_split': [2,3,5],\n",
    "#                     'clasify__min_samples_leaf': [1,2,3],\n",
    "                      'classify__max_features': ['auto', 1, 0.5]\n",
    "                     }]\n",
    "\n",
    "tune_ADAparameters = [{'features__select__k': [2,3],\n",
    "                       'classify__n_estimators': [12, 50],\n",
    "                       'classify__algorithm': ['SAMME.R', 'SAMME'],\n",
    "                       'classify__learning_rate': [1, 0.4]}\n",
    "                     ]\n",
    "\n",
    "print(\">>>>>Chosen Classifier GaussionNB()<<<<<\\n\")\n",
    "gs = GridSearchCV(pipe3, tune_GNBparameters, cv=sss)\n",
    "gs.fit(features, labels)\n",
    "clf = gs.best_estimator_\n",
    "print(\"Best score:\",gs.best_score_)\n",
    "print(\"Best parameters:\")\n",
    "print(gs.best_params_)\n",
    "\n",
    "print(\"<<<Tester Scores on Final Estimator>>>\")\n",
    "test_classifier(clf, my_dataset, my_features)\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "features_list = my_features\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
